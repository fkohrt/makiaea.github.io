<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<title>00049</title>
<script type="text/javascript">
function showhide(id)
{
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
}
</script>
<style>
rect {
transition: .6s fill;
fill: #D3D3D3;
opacity: 0;
}
rect:hover {
    fill: #D3D3D3;
    opacity: 0.2;
}

.outline {
    clear: both;
}

.svg-container {
    width: 100%;
    max-width: 4122px;
    margin-left: auto;
    margin-right: auto;
    display: block;
}

.svg-content {
    width: 100%;
}

.container {
    width: 100%;
}
</style>

</head>

<body>

<div class="container"><div class="spacer">&nbsp;</div><div class="svg-container"><svg version="1.1"  preserveAspectRatio="xMinYMin meet" class="svg-content" viewBox="0 0 4122 10094" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<image width="4122" height="10094" xlink:href="00049-assets/00049.png"></image>
<a xlink:href="00049" xlink:title="00049"><rect x="1798" y="1931" width="28" height="28"/></a>
<a xlink:href="00149" xlink:title="00149"><rect x="1798" y="2091" width="28" height="28"/></a>
<a xlink:href="00249" xlink:title="00249"><rect x="1798" y="2171" width="28" height="28"/></a>
<a xlink:href="00349" xlink:title="00349"><rect x="1798" y="2863" width="28" height="28"/></a>
<a xlink:href="00449" xlink:title="00449"><rect x="1798" y="3859" width="28" height="28"/></a>
<a xlink:href="00549" xlink:title="00549"><rect x="1798" y="4427" width="28" height="28"/></a>
<a xlink:href="00649" xlink:title="00649"><rect x="1798" y="4749" width="28" height="28"/></a>
<a xlink:href="00749" xlink:title="00749"><rect x="1798" y="4893" width="28" height="28"/></a>
<a xlink:href="00849" xlink:title="00849"><rect x="1798" y="4979" width="28" height="28"/></a>
<a xlink:href="00949" xlink:title="00949"><rect x="1798" y="5059" width="28" height="28"/></a>
<a xlink:href="01049" xlink:title="01049"><rect x="1798" y="5139" width="28" height="28"/></a>
<a xlink:href="01149" xlink:title="01149"><rect x="1798" y="5219" width="28" height="28"/></a>
<a xlink:href="01249" xlink:title="01249"><rect x="1798" y="5299" width="28" height="28"/></a>
<a xlink:href="01349" xlink:title="01349"><rect x="1798" y="5379" width="28" height="28"/></a>
<a xlink:href="01449" xlink:title="01449"><rect x="1798" y="5459" width="28" height="28"/></a>
<a xlink:href="01549" xlink:title="01549"><rect x="1798" y="5539" width="28" height="28"/></a>
<a xlink:href="01649" xlink:title="01649"><rect x="1798" y="5619" width="28" height="28"/></a>
<a xlink:href="01749" xlink:title="01749"><rect x="1798" y="5699" width="28" height="28"/></a>
<a xlink:href="01849" xlink:title="01849"><rect x="1798" y="5779" width="28" height="28"/></a>
<a xlink:href="01949" xlink:title="01949"><rect x="1798" y="5859" width="28" height="28"/></a>
<a xlink:href="02049" xlink:title="02049"><rect x="1798" y="5939" width="28" height="28"/></a>
<a xlink:href="02149" xlink:title="02149"><rect x="1798" y="6019" width="28" height="28"/></a>
<a xlink:href="02249" xlink:title="02249"><rect x="1798" y="6099" width="28" height="28"/></a>
<a xlink:href="02349" xlink:title="02349"><rect x="1798" y="6179" width="28" height="28"/></a>
<a xlink:href="02449" xlink:title="02449"><rect x="1798" y="6259" width="28" height="28"/></a>
<a xlink:href="02549" xlink:title="02549"><rect x="1798" y="6339" width="28" height="28"/></a>
<a xlink:href="02649" xlink:title="02649"><rect x="1798" y="6419" width="28" height="28"/></a>
<a xlink:href="02749" xlink:title="02749"><rect x="1798" y="6499" width="28" height="28"/></a>
<a xlink:href="02849" xlink:title="02849"><rect x="1798" y="6579" width="28" height="28"/></a>
<a xlink:href="02949" xlink:title="02949"><rect x="1798" y="6659" width="28" height="28"/></a>
<a xlink:href="03049" xlink:title="03049"><rect x="1798" y="6739" width="28" height="28"/></a>
<a xlink:href="03149" xlink:title="03149"><rect x="1798" y="6819" width="28" height="28"/></a>
<a xlink:href="03249" xlink:title="03249"><rect x="1798" y="6899" width="28" height="28"/></a>
<a xlink:href="03349" xlink:title="03349"><rect x="1798" y="6979" width="28" height="28"/></a>
<a xlink:href="03449" xlink:title="03449"><rect x="1798" y="7059" width="28" height="28"/></a>
<a xlink:href="03549" xlink:title="03549"><rect x="1798" y="7139" width="28" height="28"/></a>
<a xlink:href="03649" xlink:title="03649"><rect x="1798" y="7219" width="28" height="28"/></a>
<a xlink:href="03749" xlink:title="03749"><rect x="1798" y="7299" width="28" height="28"/></a>
<a xlink:href="03849" xlink:title="03849"><rect x="1798" y="7379" width="28" height="28"/></a>
<a xlink:href="03949" xlink:title="03949"><rect x="1798" y="7459" width="28" height="28"/></a>
<a xlink:href="04049" xlink:title="04049"><rect x="1798" y="7539" width="28" height="28"/></a>
<a xlink:href="04149" xlink:title="04149"><rect x="1798" y="7619" width="28" height="28"/></a>
<a xlink:href="04249" xlink:title="04249"><rect x="1798" y="7699" width="28" height="28"/></a>
<a xlink:href="04349" xlink:title="04349"><rect x="1798" y="7779" width="28" height="28"/></a>
<a xlink:href="04449" xlink:title="04449"><rect x="1798" y="7859" width="28" height="28"/></a>
<a xlink:href="04549" xlink:title="04549"><rect x="1798" y="7939" width="28" height="28"/></a>
<a xlink:href="04649" xlink:title="04649"><rect x="1798" y="8019" width="28" height="28"/></a>
<a xlink:href="04749" xlink:title="04749"><rect x="1798" y="8099" width="28" height="28"/></a>
<a xlink:href="04849" xlink:title="04849"><rect x="1798" y="8185" width="28" height="28"/></a>
<a xlink:href="04949" xlink:title="04949"><rect x="1798" y="8271" width="28" height="28"/></a>
<a xlink:href="05049" xlink:title="05049"><rect x="2166" y="82" width="28" height="28"/></a>
<a xlink:href="05149" xlink:title="05149"><rect x="2166" y="304" width="28" height="28"/></a>
<a xlink:href="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett" xlink:title="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett"><rect x="2412" y="271" width="556" height="67"/></a>
<a xlink:href="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett" xlink:title="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett"><rect x="2412" y="339" width="73" height="33"/></a>
<a xlink:href="05249" xlink:title="05249"><rect x="2166" y="780" width="28" height="28"/></a>
<a xlink:href="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb" xlink:title="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb"><rect x="2412" y="584" width="576" height="33"/></a>
<a xlink:href="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb" xlink:title="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb"><rect x="2412" y="618" width="431" height="33"/></a>
<a xlink:href="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world" xlink:title="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world"><rect x="2412" y="848" width="548" height="33"/></a>
<a xlink:href="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world" xlink:title="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world"><rect x="2412" y="882" width="370" height="33"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.cub.2017.12.014" xlink:title="http://dx.doi.org/10.1016/j.cub.2017.12.014"><rect x="2412" y="1011" width="527" height="33"/></a>
<a xlink:href="05349" xlink:title="05349"><rect x="2166" y="1236" width="28" height="28"/></a>
<a xlink:href="05449" xlink:title="05449"><rect x="2166" y="1478" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.7554/eLife.18103" xlink:title="http://dx.doi.org/10.7554/eLife.18103"><rect x="2412" y="1496" width="140" height="33"/></a>
<a xlink:href="05549" xlink:title="05549"><rect x="2166" y="1638" width="28" height="28"/></a>
<a xlink:href="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain" xlink:title="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain"><rect x="2794" y="1623" width="568" height="33"/></a>
<a xlink:href="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain" xlink:title="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain"><rect x="2794" y="1657" width="140" height="33"/></a>
<a xlink:href="05649" xlink:title="05649"><rect x="2166" y="1860" width="28" height="28"/></a>
<a xlink:href="http://doi.org/10.1073/pnas.1506552113" xlink:title="http://doi.org/10.1073/pnas.1506552113"><rect x="2402" y="1962" width="28" height="28"/></a>
<a xlink:href="05749" xlink:title="05749"><rect x="2166" y="2172" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1126/science.aaf6005" xlink:title="http://dx.doi.org/10.1126/science.aaf6005"><rect x="2652" y="2144" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1126/science.aaf5656" xlink:title="http://dx.doi.org/10.1126/science.aaf5656"><rect x="2662" y="2280" width="195" height="33"/></a>
<a xlink:href="05849" xlink:title="05849"><rect x="2166" y="2466" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0956797616661182" xlink:title="http://dx.doi.org/10.1177/0956797616661182"><rect x="2402" y="2552" width="28" height="28"/></a>
<a xlink:href="05949" xlink:title="05949"><rect x="2166" y="2714" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.ssresearch.2016.08.007" xlink:title="http://dx.doi.org/10.1016/j.ssresearch.2016.08.007"><rect x="2402" y="2800" width="28" height="28"/></a>
<a xlink:href="06049" xlink:title="06049"><rect x="2166" y="2884" width="28" height="28"/></a>
<a xlink:href="06149" xlink:title="06149"><rect x="2166" y="3038" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0956797616667721" xlink:title="http://dx.doi.org/10.1177/0956797616667721"><rect x="2402" y="3106" width="28" height="28"/></a>
<a xlink:href="06249" xlink:title="06249"><rect x="2166" y="3254" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1177/1368430216677304" xlink:title="http://dx.doi.org/10.1177/1368430216677304"><rect x="2402" y="3322" width="28" height="28"/></a>
<a xlink:href="06349" xlink:title="06349"><rect x="2166" y="3442" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1073/pnas.1617357114" xlink:title="http://dx.doi.org/10.1073/pnas.1617357114"><rect x="2412" y="3460" width="457" height="33"/></a>
<a xlink:href="06449" xlink:title="06449"><rect x="2166" y="3602" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.jrp.2017.03.005" xlink:title="http://dx.doi.org/10.1016/j.jrp.2017.03.005"><rect x="2412" y="3620" width="440" height="33"/></a>
<a xlink:href="06549" xlink:title="06549"><rect x="2166" y="3812" width="28" height="28"/></a>
<a xlink:href="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things" xlink:title="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things"><rect x="2412" y="3848" width="580" height="33"/></a>
<a xlink:href="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things" xlink:title="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things"><rect x="2412" y="3881" width="336" height="33"/></a>
<a xlink:href="06649" xlink:title="06649"><rect x="2166" y="4040" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1037/xge0000240" xlink:title="http://dx.doi.org/10.1037/xge0000240"><rect x="2412" y="4074" width="150" height="33"/></a>
<a xlink:href="06749" xlink:title="06749"><rect x="2166" y="4998" width="28" height="28"/></a>
<a xlink:href="https://doi.org/10.1371/journal.pcbi.1005684" xlink:title="https://doi.org/10.1371/journal.pcbi.1005684"><rect x="2412" y="5032" width="450" height="33"/></a>
<a xlink:href="06849" xlink:title="06849"><rect x="2166" y="5898" width="28" height="28"/></a>
<a xlink:href="06949" xlink:title="06949"><rect x="2166" y="5978" width="28" height="28"/></a>
<a xlink:href="07049" xlink:title="07049"><rect x="2166" y="6709" width="28" height="28"/></a>
<a xlink:href="07149" xlink:title="07149"><rect x="2166" y="7498" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1037/pspi0000114" xlink:title="http://dx.doi.org/10.1037/pspi0000114"><rect x="2412" y="7532" width="394" height="33"/></a>
<a xlink:href="07249" xlink:title="07249"><rect x="2166" y="7694" width="28" height="28"/></a>
<a xlink:href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth" xlink:title="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth"><rect x="2412" y="7694" width="568" height="33"/></a>
<a xlink:href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth" xlink:title="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth"><rect x="2412" y="7728" width="524" height="33"/></a>
<a xlink:href="07349" xlink:title="07349"><rect x="2166" y="7856" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0956797617743018" xlink:title="http://dx.doi.org/10.1177/0956797617743018"><rect x="2412" y="7857" width="556" height="33"/></a>
<a xlink:href="07449" xlink:title="07449"><rect x="2166" y="7960" width="28" height="28"/></a>
<a xlink:href="07549" xlink:title="07549"><rect x="2166" y="8040" width="28" height="28"/></a>
<a xlink:href="07649" xlink:title="07649"><rect x="2166" y="8120" width="28" height="28"/></a>
<a xlink:href="07749" xlink:title="07749"><rect x="2166" y="8200" width="28" height="28"/></a>
<a xlink:href="07849" xlink:title="07849"><rect x="2166" y="8280" width="28" height="28"/></a>
<a xlink:href="07949" xlink:title="07949"><rect x="2166" y="8360" width="28" height="28"/></a>
<a xlink:href="08049" xlink:title="08049"><rect x="2166" y="8440" width="28" height="28"/></a>
<a xlink:href="08149" xlink:title="08149"><rect x="2166" y="8520" width="28" height="28"/></a>
<a xlink:href="08249" xlink:title="08249"><rect x="2166" y="8600" width="28" height="28"/></a>
<a xlink:href="08349" xlink:title="08349"><rect x="2166" y="8680" width="28" height="28"/></a>
<a xlink:href="08449" xlink:title="08449"><rect x="2166" y="8760" width="28" height="28"/></a>
<a xlink:href="08549" xlink:title="08549"><rect x="2166" y="8840" width="28" height="28"/></a>
<a xlink:href="08649" xlink:title="08649"><rect x="2166" y="8920" width="28" height="28"/></a>
<a xlink:href="08749" xlink:title="08749"><rect x="2166" y="9000" width="28" height="28"/></a>
<a xlink:href="08849" xlink:title="08849"><rect x="2166" y="9080" width="28" height="28"/></a>
<a xlink:href="08949" xlink:title="08949"><rect x="2166" y="9160" width="28" height="28"/></a>
<a xlink:href="09049" xlink:title="09049"><rect x="2166" y="9280" width="28" height="28"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.neuron.2017.11.008" xlink:title="http://dx.doi.org/10.1016/j.neuron.2017.11.008"><rect x="2412" y="9298" width="563" height="33"/></a>
<a xlink:href="09149" xlink:title="09149"><rect x="2166" y="9400" width="28" height="28"/></a>
<a xlink:href="09249" xlink:title="09249"><rect x="2166" y="9480" width="28" height="28"/></a>
<a xlink:href="09349" xlink:title="09349"><rect x="2166" y="9560" width="28" height="28"/></a>
<a xlink:href="09449" xlink:title="09449"><rect x="2166" y="9640" width="28" height="28"/></a>
<a xlink:href="09549" xlink:title="09549"><rect x="2166" y="9720" width="28" height="28"/></a>
<a xlink:href="09649" xlink:title="09649"><rect x="2166" y="9800" width="28" height="28"/></a>
<a xlink:href="09749" xlink:title="09749"><rect x="2166" y="9880" width="28" height="28"/></a>
<a xlink:href="09849" xlink:title="09849"><rect x="2166" y="9960" width="28" height="28"/></a>
<a xlink:href="09949" xlink:title="09949"><rect x="2166" y="10040" width="28" height="28"/></a>
</svg>
</div>
</div>

<br/>
<a href="javascript:showhide('outlineDiv')">Outline show/hide</a>
<div id="outlineDiv" style="display:none;">
<h1>00049</h1>
<p>candy</p>
<h2>05049</h2>
<h2>05149</h2>
<ul>
<li>
<p>neuroscientist lisa feldman barrett explains how emotions are made 2017<br />
<a href="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett">theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett</a></p>
<ul>
<li>I think understanding how emotions are constructed widens the horizon of control. You realize that if your brain is using your past to construct your present, you can invest energy in the present to cultivate new experiences that then become the seeds for your future. You can cultivate or curate experiences in the now and then they become, if you practice them, they become automated enough that your brain will automatically construct them in the future.</li>
</ul>
</li>
</ul>
<h2>05249</h2>
<ul>
<li>
<p><em>the vision thing: how babies colour in the world</em><br />
nicola davis 2017<br />
<a href="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb">theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world</a></p>
</li>
<li>
<p><em>the sea was never blue</em><br />
the greek colour experience was made of movement and shimmer. can we ever glimpse what they saw when gazing out to sea?<br />
maria michela sassi 2017<br />
<a href="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world">aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world</a></p>
</li>
<li>
<p><em>hunter-gatherer olfaction is special</em><br />
majid and kruspe 2018<br />
<a href="http://dx.doi.org/10.1016/j.cub.2017.12.014">http://dx.doi.org/10.1016/j.cub.2017.12.014</a></p>
<ul>
<li>
<p>•People struggle to name odors, but this limitation is not universal<br />
•Is superior olfactory performance due to subsistence, ecology or language family?<br />
•Hunter-gatherers and non-hunter-gatherers from the same environment were compared<br />
•Only hunter-gatherers were proficient odor namers, showing subsistence is crucial</p>
<p>People struggle to name odors. This has been attributed to a diminution of olfaction in trade-off to vision. This presumption has been challenged recently by data from the hunter-gatherer Jahai who, unlike English speakers, find odors as easy to name as colors. Is the superior olfactory performance among the Jahai because of their ecology (tropical rainforest), their language family (Aslian), or because of their subsistence (they are hunter-gatherers)? We provide novel evidence from the hunter-gatherer Semaq Beri and the non-hunter-gatherer (swidden-horticulturalist) Semelai that subsistence is the critical factor. Semaq Beri and Semelai speakers—who speak closely related languages and live in the tropical rainforest of the Malay Peninsula—took part in a controlled odor- and color-naming experiment. The swidden-horticulturalist Semelai found odors much more difficult to name than colors, replicating the typical Western finding. But for the hunter-gatherer Semaq Beri odor naming was as easy as color naming, suggesting that hunter-gatherer olfactory cognition is special.</p>
</li>
</ul>
</li>
</ul>
<h2>05349</h2>
<ul>
<li>“experiences vs things”
<ul>
<li>
<p><em>stuffocation</em><br />
james wallman 2013<br />
978-0-9575245-2-1</p>
</li>
<li>
<p>“experiences are more prone to positive reinterpretation, less likely to be dulled by hedonic adaptation, harder to compare, more likely to contribute to identity, and they bring you closer to people.”</p>
</li>
</ul>
</li>
</ul>
<h2>05449</h2>
<ul>
<li>
<p><em>unexpected arousal modulates the influence of sensory noise on confidence</em><br />
micah allen et al. 2016<br />
<a href="http://dx.doi.org/10.7554/eLife.18103">eLife.18103</a></p>
<ul>
<li>“relevant to understanding clinical disorders, such as anxiety and depression, where changes in arousal might lock sufferers into an unrealistically certain or uncertain world”</li>
<li>Human perception is invariably accompanied by a graded feeling of confidence that guides metacognitive awareness and decision-making. It is often assumed that this arises solely from the feed-forward encoding of the strength or precision of sensory inputs. In contrast, interoceptive inference models suggest that confidence reflects a weighted integration of sensory precision and expectations about internal states, such as arousal. Here we test this hypothesis using a novel psychophysical paradigm, in which unseen disgust-cues induced unexpected, unconscious arousal just before participants discriminated motion signals of variable precision. Across measures of perceptual bias, uncertainty, and physiological arousal we found that arousing disgust cues modulated the encoding of sensory noise. Furthermore, the degree to which trial-by-trial pupil fluctuations encoded this nonlinear interaction correlated with trial level confidence. Our results suggest that unexpected arousal regulates perceptual precision, such that subjective confidence reflects the integration of both external sensory and internal, embodied states.</li>
</ul>
</li>
</ul>
<h2>05549</h2>
<ul>
<li>supernormal stimuli
<ul>
<li><em>pikachu is a chocolate milkshake</em><br />
joel frohlich 2016<br />
<a href="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain">how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain</a></li>
</ul>
</li>
</ul>
<h2>05649</h2>
<ul>
<li>
<p><em>covert digital manipulation of vocal emotion alter speakers’ emotional states in a congruent direction</em><br />
jean-julien aucouturier, petter johansson, lars hall, rodrigo segnini, lolita mercadié, and katsumi watanabe 2016<br />
http://doi.org/10.1073/pnas.1506552113<br />
Link: <a href="http://doi.org/10.1073/pnas.1506552113">doi.org/10.1073/pnas.1506552113</a></p>
<ul>
<li>
<p><em>Significance</em><br />
We created a digital audio platform to covertly modify the emotional tone of participants’ voices while they talked toward happiness, sadness, or fear. Independent listeners perceived the transformations as natural examples of emotional speech, but the participants remained unaware of the manipulation, indicating that we are not continuously monitoring our own emotional signals. Instead, as a consequence of listening to their altered voices, the emotional state of the participants changed in congruence with the emotion portrayed. This result is the first evidence, to our knowledge, of peripheral feedback on emotional experience in the auditory domain. This finding is of great significance, because the mechanisms behind the production of vocal emotion are virtually unknown.</p>
</li>
<li>
<p><em>Abstract</em><br />
Research has shown that people often exert control over their emotions. By modulating expressions, reappraising feelings, and redirecting attention, they can regulate their emotional experience. These findings have contributed to a blurring of the traditional boundaries between cognitive and emotional processes, and it has been suggested that emotional signals are produced in a goal-directed way and monitored for errors like other intentional actions. However, this interesting possibility has never been experimentally tested. To this end, we created a digital audio platform to covertly modify the emotional tone of participants’ voices while they talked in the direction of happiness, sadness, or fear. The result showed that the audio transformations were being perceived as natural examples of the intended emotions, but the great majority of the participants, nevertheless, remained unaware that their own voices were being manipulated. This finding indicates that people are not continuously monitoring their own voice to make sure that it meets a predetermined emotional target. Instead, as a consequence of listening to their altered voices, the emotional state of the participants changed in congruence with the emotion portrayed, which was measured by both self-report and skin conductance level. This change is the first evidence, to our knowledge, of peripheral feedback effects on emotional experience in the auditory domain. As such, our result reinforces the wider framework of self-perception theory: that we often use the same inferential strategies to understand ourselves as those that we use to understand others.</p>
</li>
</ul>
</li>
</ul>
<h2>05749</h2>
<ul>
<li>sensing
<ul>
<li>
<p><em>phytochromes function as thermosensors in arabidopsis</em><br />
jae-hoon jung et al. 2016<br />
http://dx.doi.org/10.1126/science.aaf6005<br />
Link: <a href="http://dx.doi.org/10.1126/science.aaf6005">dx.doi.org/10.1126/science.aaf6005</a></p>
<ul>
<li>Plants are responsive to temperature, and can distinguish differences of 1°C. In <em>Arabidopsis</em>, warmer temperature accelerates flowering and increases elongation growth (thermomorphogenesis). The mechanisms of temperature perception are however largely unknown. We describe a major thermosensory role for the phytochromes (red light receptors) during the night. Phytochrome null plants display a constitutive warm temperature response, and consistent with this, we show in this background that the warm temperature transcriptome becomes de-repressed at low temperatures. We have discovered phytochrome B (phyB) directly associates with the promoters of key target genes in a temperature dependent manner. The rate of phyB inactivation is proportional to temperature in the dark, enabling phytochromes to function as thermal timers, integrating temperature information over the course of the night.</li>
</ul>
</li>
<li>
<p><em>phytochrome b integrates light and temperature signals in arabidopsis</em><br />
martina legris et al. 2016<br />
<a href="http://dx.doi.org/10.1126/science.aaf5656">science.aaf5656</a></p>
<ul>
<li>Ambient temperature regulates many aspects of plant growth and development but its sensors are unknown. Here, we demonstrate that the phytochrome B (phyB) photoreceptor participates in temperature perception through its temperature-dependent reversion from the active Pfr state to the inactive Pr state. Increased rates of thermal reversion upon exposing <em>Arabidopsis</em> seedlings to warm environments reduce both the abundance of the biologically active Pfr-Pfr dimer pool of phyB and the size of the associated nuclear bodies, even in daylight. Mathematical analysis of stem growth for seedlings expressing wild-type phyB or thermally stable variants under various combinations of light and temperature revealed that phyB is physiologically responsive to both signals. We therefore propose that in addition to its photoreceptor functions, phyB is a temperature sensor in plants.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>05849</h2>
<ul>
<li>
<p><em>young children see a single action and infer a social norm: promiscuous normativity in 3-year-olds</em><br />
marco f. h. schmidt, lucas p. butler, julia heinz, and michael tomasello 2016<br />
http://dx.doi.org/10.1177/0956797616661182<br />
Link: <a href="http://dx.doi.org/10.1177/0956797616661182">dx.doi.org/10.1177/0956797616661182</a></p>
<ul>
<li>Human social life depends heavily on social norms that prescribe and proscribe specific actions. Typically, young children learn social norms from adult instruction. In the work reported here, we showed that this is not the whole story: Three-year-old children are promiscuous normativists. In other words, they spontaneously inferred the presence of social norms even when an adult had done nothing to indicate such a norm in either language or behavior. And children of this age even went so far as to enforce these self-inferred norms when third parties “broke” them. These results suggest that children do not just passively acquire social norms from adult behavior and instruction; rather, they have a natural and proactive tendency to go from “is” to “ought.” That is, children go from observed actions to prescribed actions and do not perceive them simply as guidelines for their own behavior but rather as objective normative rules applying to everyone equally.</li>
</ul>
</li>
</ul>
<h2>05949</h2>
<ul>
<li>
<p><em>the racialized construction of exceptionality: experimental evidence of race/ethnicity effects on teachers' interventions</em><br />
rachel elizabeth fish 2016<br />
http://dx.doi.org/10.1016/j.ssresearch.2016.08.007<br />
Link: <a href="http://dx.doi.org/10.1016/j.ssresearch.2016.08.007">dx.doi.org/10.1016/j.ssresearch.2016.08.007</a></p>
<ul>
<li>Scholars, policy-makers, and practitioners have long argued that students of color are over-represented in special education and under-represented in gifted education, arguing that educators make racially/ethnically biased decisions to refer and qualify students with disabilities and giftedness. Recent research has called this into question, focusing on the role of confounders of race/ethnicity. However, the role of educator decisions in the disproportionality is still unclear. In this study, I examine the role of student race/ethnicity in teachers' categorization of student needs as “exceptional” and in need of special or gifted education services. I use an original survey experiment in which teachers read case studies of fictional male students in which the race/ethnicity, English Language Learner status, and exceptionality characteristics were experimentally manipulated. The teachers are then asked whether they would refer the student for exceptionality testing. My findings suggest a complex intersection of race/ethnicity and exceptionality, in which white boys are more likely to be suspected of having exceptionalities when they exhibit academic challenges, while boys of color are more likely to be suspected when they exhibit behavioral challenges. This suggests that the racialized construction of exceptionalities reflects differential academic expectations and interpretations of behavior by race/ethnicity, with implications for the subjectivity of exceptionality identification and for the exacerbation of racial/ethnic inequalities in education.</li>
</ul>
</li>
</ul>
<h2>06049</h2>
<ul>
<li><em>how to lie with statistics</em><br />
darrel huff 1954</li>
</ul>
<h2>06149</h2>
<ul>
<li>
<p><em>social class and the motivational relevance of other human beings: evidence from visual attention</em><br />
p. dietze, e. d. knowles 2016<br />
http://dx.doi.org/10.1177/0956797616667721<br />
Link: <a href="http://dx.doi.org/10.1177/0956797616667721">dx.doi.org/10.1177/0956797616667721</a></p>
<ul>
<li>We theorize that people’s social class affects their appraisals of others’ motivational relevance—the degree to which others are seen as potentially rewarding, threatening, or otherwise worth attending to. Supporting this account, three studies indicate that social classes differ in the amount of attention their members direct toward other human beings. In Study 1, wearable technology was used to film the visual fields of pedestrians on city streets; higher-class participants looked less at other people than did lower-class participants. In Studies 2a and 2b, participants’ eye movements were tracked while they viewed street scenes; higher class was associated with reduced attention to people in the images. In Study 3, a change-detection procedure assessed the degree to which human faces spontaneously attract visual attention; faces proved less effective at drawing the attention of high-class than low-class participants, which implies that class affects spontaneous relevance appraisals. The measurement and conceptualization of social class are discussed.</li>
</ul>
</li>
</ul>
<h2>06249</h2>
<ul>
<li>
<p><em>the threat of increasing diversity: why many white americans support trump in the 2016 presidential election</em><br />
b. major, a. blodorn, g. major blascovich 2016<br />
http://dx.doi.org/10.1177/1368430216677304<br />
Link: <a href="http://dx.doi.org/10.1177/1368430216677304">dx.doi.org/10.1177/1368430216677304</a></p>
<ul>
<li>What accounts for the widespread support for Donald Trump in the 2016 U.S. presidential race? This experiment demonstrates that the changing racial demographics of America contribute to Trump’s success as a presidential candidate among White Americans whose race/ethnicity is central to their identity. Reminding White Americans high in ethnic identification that non-White racial groups will outnumber Whites in the United States by 2042 caused them to become more concerned about the declining status and influence of White Americans as a group (i.e., experience group status threat), and caused them to report increased support for Trump and anti-immigrant policies, as well as greater opposition to political correctness. Increased group status threat mediated the effects of the racial shift condition on candidate support, anti-immigrant policy support, and opposition to political correctness. Among Whites low in ethnic identification, in contrast, the racial shift condition had no effect on group status threat or support for anti-immigrant policies, but did cause decreased positivity toward Trump and decreased opposition to political correctness. Group status threat did not mediate these effects. Reminders of the changing racial demographics had comparable effects for Democrats and Republicans. Results illustrate the importance of changing racial demographics and White ethnic identification in voter preferences and how social psychological theory can illuminate voter preferences.</li>
</ul>
</li>
</ul>
<h2>06349</h2>
<ul>
<li>
<p><em>behavioral and neural correlates to multisensory detection of sick humans</em><br />
christina regenbogen et al. 2017<br />
<a href="http://dx.doi.org/10.1073/pnas.1617357114">dx.doi.org/10.1073/pnas.1617357114</a></p>
<ul>
<li>
<p>In the perpetual race between evolving organisms and pathogens, the human immune system has evolved to reduce the harm of infections. As part of such a system, avoidance of contagious individuals would increase biological fitness. The present study shows that we can detect both facial and olfactory cues of sickness in others just hours after experimental activation of their immune system. The study further demonstrates that multisensory integration of these olfactory and visual sickness cues is a crucial mechanism for how we detect and socially evaluate sick individuals. Thus, by motivating the avoidance of sick conspecifics, olfactory–visual cues, both in isolation and integrated, may be important parts of circuits handling imminent threats of contagion.</p>
<p>Throughout human evolution, infectious diseases have been a primary cause of death. Detection of subtle cues indicating sickness and avoidance of sick conspecifics would therefore be an adaptive way of coping with an environment fraught with pathogens. This study determines how humans perceive and integrate early cues of sickness in conspecifics sampled just hours after the induction of immune system activation, and the underlying neural mechanisms for this detection. In a double-blind placebo-controlled crossover design, the immune system in 22 sample donors was transiently activated with an endotoxin injection [lipopolysaccharide (LPS)]. Facial photographs and body odor samples were taken from the same donors when “sick” (LPS-injected) and when “healthy” (saline-injected) and subsequently were presented to a separate group of participants (<em>n</em> = 30) who rated their liking of the presented person during fMRI scanning. Faces were less socially desirable when sick, and sick body odors tended to lower liking of the faces. Sickness status presented by odor and facial photograph resulted in increased neural activation of odor- and face-perception networks, respectively. A superadditive effect of olfactory–visual integration of sickness cues was found in the intraparietal sulcus, which was functionally connected to core areas of multisensory integration in the superior temporal sulcus and orbitofrontal cortex. Taken together, the results outline a disease-avoidance model in which neural mechanisms involved in the detection of disease cues and multisensory integration are vital parts.</p>
</li>
</ul>
</li>
</ul>
<h2>06449</h2>
<ul>
<li>
<p><em>seeing it both ways: openness to experience and binocular rivalry suppression</em><br />
anna antinori, olivia l. carter, luke d. smillie 2017<br />
<a href="http://dx.doi.org/10.1016/j.jrp.2017.03.005">dx.doi.org/10.1016/j.jrp.2017.03.005</a></p>
<ul>
<li>
<p>Demonstrates personality and mood can impact low-level perceptual experiences.<br />
Mixed percept, a binocular rivalry state, positively correlated with openness.<br />
Findings were replicated across samples and response bias was excluded.<br />
Used a perceptual-aesthetic mood induction that increased mixed in open people.</p>
<p>Openness to experience is characterised by flexible and inclusive cognition. Here we investigated whether this extends to basic visual perception, such that open people combine information more flexibly, even at low-levels of perceptual processing. We used binocular rivalry, where the brain alternates between perceptual solutions and times where neither solution is fully suppressed, mixed percept. Study 1 showed that openness is positively associated with duration of mixed percept and ruled out the possibility of response bias. Study 2 showed that mixed percept increased following a positive mood induction particularly for open people. Overall, the results showed that openness is linked to differences in low-level visual perceptual experience. Further studies should investigate whether this may be driven by common neural processes.</p>
</li>
</ul>
</li>
</ul>
<h2>06549</h2>
<ul>
<li><em>now you see it, now you… seeing things that are hidden; failing to see things in plain sight. how magic exploits the everyday weirdness of perception</em><br />
vebjørn ekroll 2017<br />
<a href="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things">aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things</a></li>
</ul>
<h2>06649</h2>
<ul>
<li>
<p><em>the order of disorder: deconstructing visual disorder and its effect on rule-breaking</em><br />
hiroki p. kotabe, omid kardan, marc g. berman 2016<br />
<a href="http://dx.doi.org/10.1037/xge0000240">xge0000240</a></p>
<ul>
<li>Disorderly environments are linked to disorderly behaviors. Broken windows theory (Wilson &amp; Kelling, 1982), an influential theory of crime and rule-breaking, assumes that scene-level social disorder cues (e.g., litter, graffiti) cause people to reason that they can get away with breaking rules. But what if part of the story is not about such complex social reasoning? Recent research suggests that basic visual disorder cues may be sufficient to encourage complex rule-breaking behavior. To test this hypothesis, we first conducted a set of experiments (Experiments 1–3) in which we identified basic visual disorder cues that generalize across visual stimuli with a variety of semantic content. Our results revealed that spatial features (e.g., nonstraight edges, asymmetry) are more important than color features (e.g., hue, saturation, value) for visual disorder. Exploiting this knowledge, we then reconstructed stimuli contrasted in terms of visual disorder, but absent of scene-level social disorder cues, to test whether visual disorder alone encourages cheating in a second set of experiments (Experiments 4 and 5). In these experiments, manipulating visual disorder increased the likelihood of cheating by up to 35% and the average magnitude of cheating by up to 87%. This work suggests that theories of rule-breaking that assume that complex social reasoning (e.g., about norms, policing, poverty) is necessary, should be reconsidered (e.g., Kelling &amp; Coles, 1997; Sampson &amp; Raudenbush, 2004). Furthermore, these experiments show that simple perceptual properties of the environment can affect complex behavior and sheds light on the extent to which our actions are within our control.</li>
</ul>
</li>
</ul>
<h2>06749</h2>
<ul>
<li>
<p><em>confirmation bias in human reinforcement learning: evidence from counterfactual feedback processing</em><br />
stefano palminteri et al. 2017<br />
<a href="https://doi.org/10.1371/journal.pcbi.1005684">doi.org/10.1371/journal.pcbi.1005684</a></p>
<ul>
<li>
<p>Previous studies suggest that <em>factual</em> learning, that is, learning from obtained outcomes, is biased, such that participants preferentially take into account positive, as compared to negative, prediction errors. However, whether or not the prediction error valence also affects <em>counterfactual</em> learning, that is, learning from forgone outcomes, is unknown. To address this question, we analysed the performance of two groups of participants on reinforcement learning tasks using a computational model that was adapted to test if prediction error valence influences learning. We carried out two experiments: in the factual learning experiment, participants learned from partial feedback (i.e., the outcome of the chosen option only); in the counterfactual learning experiment, participants learned from complete feedback information (i.e., the outcomes of both the chosen and unchosen option were displayed). In the factual learning experiment, we replicated previous findings of a valence-induced bias, whereby participants learned preferentially from positive, relative to negative, prediction errors. In contrast, for counterfactual learning, we found the opposite valence-induced bias: negative prediction errors were preferentially taken into account, relative to positive ones. When considering valence-induced bias in the context of both factual and counterfactual learning, it appears that people tend to preferentially take into account information that confirms their current choice.</p>
<p>While the investigation of decision-making biases has a long history in economics and psychology, learning biases have been much less systematically investigated. This is surprising as most of the choices we deal with in everyday life are recurrent, thus allowing learning to occur and therefore influencing future decision-making. Combining behavioural testing and computational modeling, here we show that the valence of an outcome biases both factual and counterfactual learning. When considering factual and counterfactual learning together, it appears that people tend to preferentially take into account information that confirms their current choice. Increasing our understanding of learning biases will enable the refinement of existing models of value-based decision-making.</p>
</li>
</ul>
</li>
</ul>
<h2>06849</h2>
<h2>06949</h2>
<h2>07049</h2>
<ul>
<li>
<p><em>pre–suasion: a revolutionary way to influence and persuade</em><br />
robert cialdini 2016<br />
978-1-5011-0981-2</p>
<ul>
<li>the effectiveness of persuasive messages will be drastically affected by the type of opener experienced immediately in advance</li>
<li>Put people in a wary state of mind via that opener, and, driven by a desire for safety, a popularity-based appeal will soar, whereas a distinctiveness-based appeal will sink. But use it to put people in an amorous state of mind, and, driven by a consequent desire to stand out, the reverse will occur.</li>
<li>salience–importance confounding</li>
<li>focal–causal confounding</li>
<li>orientation reflex
<ul>
<li>“a persuasion-oriented producer, writer, or director needs to be concerned principally with shots and cuts”</li>
<li>“cuts are crucial to persuasive success because they can be manipulated to bring into focus the feature of a message the persuader believes to be most convincing—by shifting the scene to that feature. That cut will instigate an orienting response to the winning feature in audience members’ brains before they even experience it.”</li>
<li>create background of similar things draws attention to that which is different and hence perceived importance</li>
</ul>
</li>
<li>unresolved
<ul>
<li>mystery</li>
</ul>
</li>
<li>“the main purpose of speech is to direct listeners’ attention to a selected sector of reality. Once that is accomplished, the listeners’ existing associations to the now-spotlighted sector will take over to determine the reaction.”</li>
<li><em>influence: science and practice</em><br />
robert cialdini</li>
</ul>
</li>
</ul>
<h2>07149</h2>
<ul>
<li>
<p><em>perceived entitlement causes discrimination against attractive job candidates in the domain of relatively less desirable jobs</em><br />
margaret lee et al. 2017<br />
<a href="http://dx.doi.org/10.1037/pspi0000114">dx.doi.org/10.1037/pspi0000114</a></p>
<ul>
<li>People generally hold positive stereotypes of physically attractive people and because of those stereotypes often treat them more favorably. However, we propose that some beliefs about attractive people, specifically, the perception that attractive individuals have a greater sense of entitlement than less attractive individuals, can result in negative treatment of attractive people. We examine this in the context of job selection and propose that for relatively less desirable jobs, attractive candidates will be discriminated against. We argue that the ascribed sense of entitlement to good outcomes leads to perceptions that attractive individuals are more likely to be dissatisfied working in relatively less desirable jobs. When selecting candidates for relatively less desirable jobs, decision makers try to ascertain whether a candidate would be satisfied in those jobs, and the stereotype of attractive individuals feeling entitled to good outcomes makes decision makers judge attractive candidates as more likely to be dissatisfied in relatively less (but not more) desirable jobs. Consequently, attractive candidates are discriminated against in the selection for relatively less desirable jobs. Four experiments found support for this theory. Our results suggest that different discriminatory processes operate when decision makers select among candidates for relatively less desirable jobs and that attractive people might be systematically discriminated against in a segment of the workforce.</li>
</ul>
</li>
</ul>
<h2>07249</h2>
<ul>
<li><em>’fiction is outperforming reality’: how youtube’s algorithm distorts truth</em><br />
paul lewis 2018<br />
<a href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth">https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth</a></li>
</ul>
<h2>07349</h2>
<ul>
<li>
<p><em>déjà vu: an illusion of prediction</em><br />
anne m. cleary, alexander b. claxton 2018<br />
<a href="http://dx.doi.org/10.1177/0956797617743018">http://dx.doi.org/10.1177/0956797617743018</a></p>
<ul>
<li>Déjà vu is beginning to be scientifically understood as a memory phenomenon. Despite recent scientific advances, a remaining puzzle is the purported association between déjà vu and feelings of premonition. Building on research showing that déjà vu can be driven by an unrecalled memory of a past experience that relates to the current situation, we sought evidence of memory-based predictive ability during déjà vu states. Déjà vu did not lead to above-chance ability to predict the next turn in a navigational path resembling a previously experienced but unrecalled path (although such resemblance increased reports of déjà vu). However, déjà vu states were accompanied by increased feelings of knowing the direction of the next turn. The results suggest that feelings of premonition during déjà vu occur and can be illusory. Metacognitive bias brought on by the state itself may explain the peculiar association between déjà vu and the feeling of premonition.</li>
</ul>
</li>
</ul>
<h2>07449</h2>
<h2>07549</h2>
<h2>07649</h2>
<h2>07749</h2>
<h2>07849</h2>
<h2>07949</h2>
<h2>08049</h2>
<h2>08149</h2>
<h2>08249</h2>
<h2>08349</h2>
<h2>08449</h2>
<h2>08549</h2>
<h2>08649</h2>
<h2>08749</h2>
<h2>08849</h2>
<h2>08949</h2>
<h2>09049</h2>
<ul>
<li>
<p><em>mechanosensory-based phase coding of odor identity in the olfactory bulb</em><br />
ryo iwata et al. 2017<br />
<a href="http://dx.doi.org/10.1016/j.neuron.2017.11.008">http://dx.doi.org/10.1016/j.neuron.2017.11.008</a></p>
<ul>
<li>
<p>•Mechanosensation in olfactory sensory neurons generates sniff-coupled oscillations<br />
•Phase coding in mitral/tufted cells distinguishes odor from mechanical signals<br />
•Phase coding is more stable than rate coding across time and odor concentrations<br />
•The loss of mechanosensory-based oscillations impairs robust phase coding of odors</p>
<p>Mitral and tufted (M/T) cells in the olfactory bulb produce rich temporal patterns of activity in response to different odors. However, it remains unknown how these temporal patterns are generated and how they are utilized in olfaction. Here we show that temporal patterning effectively discriminates between the two sensory modalities detected by olfactory sensory neurons (OSNs): odor and airflow-driven mechanical signals. Sniff-induced mechanosensation generates glomerulus-specific oscillatory activity in M/T cells, whose phase was invariant across airflow speed. In contrast, odor stimulation caused phase shifts (phase coding). We also found that odor-evoked phase shifts are concentration invariant and stable across multiple sniff cycles, contrary to the labile nature of rate coding. The loss of oscillatory mechanosensation impaired the precision and stability of phase coding, demonstrating its role in olfaction. We propose that phase, not rate, coding is a robust encoding strategy of odor identity and is ensured by airflow-induced mechanosensation in OSNs.</p>
</li>
</ul>
</li>
</ul>
<h2>09149</h2>
<h2>09249</h2>
<h2>09349</h2>
<h2>09449</h2>
<h2>09549</h2>
<h2>09649</h2>
<h2>09749</h2>
<h2>09849</h2>
<h2>09949</h2>
<h2>04949</h2>
<h2>04849</h2>
<ul>
<li>Huxley had a similar–sounding visual recall issue as I have.</li>
</ul>
<h2>04749</h2>
<h2>04649</h2>
<h2>04549</h2>
<h2>04449</h2>
<h2>04349</h2>
<h2>04249</h2>
<h2>04149</h2>
<h2>04049</h2>
<h2>03949</h2>
<h2>03849</h2>
<h2>03749</h2>
<h2>03649</h2>
<h2>03549</h2>
<h2>03449</h2>
<h2>03349</h2>
<h2>03249</h2>
<h2>03149</h2>
<h2>03049</h2>
<h2>02949</h2>
<h2>02849</h2>
<h2>02749</h2>
<h2>02649</h2>
<h2>02549</h2>
<h2>02449</h2>
<h2>02349</h2>
<h2>02249</h2>
<h2>02149</h2>
<h2>02049</h2>
<h2>01949</h2>
<h2>01849</h2>
<h2>01749</h2>
<h2>01649</h2>
<h2>01549</h2>
<h2>01449</h2>
<h2>01349</h2>
<h2>01249</h2>
<h2>01149</h2>
<h2>01049</h2>
<h2>00949</h2>
<h2>00849</h2>
<h2>00749</h2>
<ul>
<li>a person can be greatly aware one moment but almost unaware the next</li>
</ul>
<h2>00649</h2>
<ul>
<li>there are no absolutes, no truly right, no truly wrong. we should not judge ourselves or others harshly — for even our best intentions and plans may not be as close to responsible as we thought they were.</li>
</ul>
<h2>00549</h2>
<ul>
<li>
<p><em>world too small</em><br />
when someone's world becomes too small, that is when problems start. They start to attribute even accidental consequences to alleged malicious behaviour in the people around them.</p>
</li>
<li>
<p>our perspective sometimes narrows, and it is then that we are more likely to abuse.</p>
</li>
<li>
<p>when our perspective narrows, it becomes harder to break out of our current outlook, to learn of the changing shape of reality, to look beyond what we already believe.</p>
</li>
</ul>
<h2>00449</h2>
<ul>
<li>dyslexia
<ul>
<li>dyslexic in dream: I could read the numbers but couldn't quite make out the letters; apparently this is what happens in dyslexia. could it be that in dreams I am dyslexic? apparently not all people are dyslexic in their dreams, at least not all the time. so could it be that when a certain part of the brain is resting in sleep, it resembles dyslexia?</li>
<li>could it be that dream states are because certain parts of the brain are resting and we may be able to correlate different types of dream states with different parts of the brain resting?</li>
<li>maybe dyslexics have a certain kind of synaesthesia which mixes letters with colours and movement. when colorised filters applied, the colour no longer salient so movement also suppressed. the type of coloured filter required to block the so-called dyslexia varies with the kind of so-called synaesthesia.</li>
</ul>
</li>
</ul>
<h2>00349</h2>
<ul>
<li>Instinct
<ul>
<li>I have a strange premonition that something is going to happen soon in the world, and we won't be able to contact each other. I want you to know I will be thinking of you. here are my notes so far, I don't know if they'll be any help…</li>
<li>when I was young, I felt intuitively that something was awfully wrong with our society. after another twenty years I understood how to communicate this in language</li>
<li>trust your instincts. if they tell you to be wary, that's okay.</li>
<li>Instinct is a name given to processes we do not yet understand. Perception may be a better name for it. We can perceive and act on a situation even if we don't understand how to explain it in words.</li>
<li>Test your perception and then trust it. A way to approach this is to try to understand the physical filters on your perception, and your mental filters such as bias and judgement. Then we can understand some of the obstacles in the way of our perception and can trust our own perception.</li>
<li>people minimise what they don't understand. most want conscious explanation of hunches, feelings, premonitions before they can actually be articulated by our conscious brains, that is the very definition of premonition. they devalue this because they don't understand it and don't trust that we can understand something is useful when they can't. they demand that we trust them but they need to uphold the corresponding reponsibility. at the moment they don't so it is why we don't like it even though we could not find the words to express it before
<ul>
<li>a crucial difference in outlook and expectation that exists and is self–perpetuating</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>00249</h2>
<h2>00149</h2>
<h2>00049</h2>
<ul>
<li>indium In (indigo, from blue spectrum lines)</li>
<li>candy smith–foster
<ul>
<li>crossing a railway bridge</li>
<li>parrot</li>
</ul>
</li>
<li>DI</li>
</ul>

</div>

</body>
</html>
