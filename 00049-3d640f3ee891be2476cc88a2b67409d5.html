<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<title>3d640f3ee891be2476cc88a2b67409d5</title>
<script type="text/javascript">
function showhide(id)
{
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
}
</script>
<style>
rect {
transition: .6s fill;
fill: #D3D3D3;
opacity: 0;
}
rect:hover {
    fill: #D3D3D3;
    opacity: 0.2;
}

.outline {
    clear: both;
}

.svg-container {
    width: 100%;
    max-width: 1698px;
    margin-left: auto;
    margin-right: auto;
    display: block;
}

.svg-content {
    width: 100%;
}

.container {
    width: 100%;
}
</style>

</head>

<body>

<div class="container"><div class="spacer">&nbsp;</div><div class="svg-container"><svg version="1.1"  preserveAspectRatio="xMinYMin meet" class="svg-content" viewBox="0 0 1698 8192" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<image width="1698" height="8192" xlink:href="00049-3d640f3ee891be2476cc88a2b67409d5-assets/00049-3d640f3ee891be2476cc88a2b67409d5.png"></image>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05049"><rect x="878" y="307" width="12" height="12"/></a>
<a xlink:href="https://aeon.co/essays/are-humans-really-blind-to-the-gorilla-on-the-basketball-court" xlink:title="https://aeon.co/essays/are-humans-really-blind-to-the-gorilla-on-the-basketball-court"><rect x="1128" y="95" width="259" height="15"/></a>
<a xlink:href="https://aeon.co/essays/are-humans-really-blind-to-the-gorilla-on-the-basketball-court" xlink:title="https://aeon.co/essays/are-humans-really-blind-to-the-gorilla-on-the-basketball-court"><rect x="1128" y="111" width="195" height="15"/></a>
<a xlink:href="https://doi.org/10.3758/s13423-016-1198-z" xlink:title="https://doi.org/10.3758/s13423-016-1198-z"><rect x="1128" y="168" width="242" height="15"/></a>
<a xlink:href="https://doi.org/10.1287/stsc.2017.0048" xlink:title="https://doi.org/10.1287/stsc.2017.0048"><rect x="1128" y="243" width="217" height="15"/></a>
<a xlink:href="https://doi.org/10.3758/s13423-017-1333-5" xlink:title="https://doi.org/10.3758/s13423-017-1333-5"><rect x="1128" y="316" width="243" height="15"/></a>
<a xlink:href="https://www.inverse.com/article/48300-why-is-it-hard-to-focus-research-humans" xlink:title="https://www.inverse.com/article/48300-why-is-it-hard-to-focus-research-humans"><rect x="1121" y="405" width="264" height="15"/></a>
<a xlink:href="https://www.inverse.com/article/48300-why-is-it-hard-to-focus-research-humans" xlink:title="https://www.inverse.com/article/48300-why-is-it-hard-to-focus-research-humans"><rect x="1121" y="421" width="170" height="15"/></a>
<a xlink:href="https://doi.org/10.1016/j.neuron.2018.07.032" xlink:title="https://doi.org/10.1016/j.neuron.2018.07.032"><rect x="1121" y="495" width="248" height="15"/></a>
<a xlink:href="https://doi.org/10.1016/j.neuron.2018.07.038" xlink:title="https://doi.org/10.1016/j.neuron.2018.07.038"><rect x="1121" y="568" width="248" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05149"><rect x="878" y="680" width="12" height="12"/></a>
<a xlink:href="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett" xlink:title="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett"><rect x="965" y="665" width="255" height="31"/></a>
<a xlink:href="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett" xlink:title="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett"><rect x="965" y="696" width="33" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05249"><rect x="878" y="972" width="12" height="12"/></a>
<a xlink:href="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb" xlink:title="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb"><rect x="965" y="809" width="264" height="15"/></a>
<a xlink:href="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb" xlink:title="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb"><rect x="965" y="824" width="198" height="15"/></a>
<a xlink:href="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world" xlink:title="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world"><rect x="965" y="930" width="251" height="15"/></a>
<a xlink:href="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world" xlink:title="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world"><rect x="965" y="946" width="170" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.cub.2017.12.014" xlink:title="http://dx.doi.org/10.1016/j.cub.2017.12.014"><rect x="965" y="1005" width="242" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1038/s41598-018-25433-5" xlink:title="http://dx.doi.org/10.1038/s41598-018-25433-5"><rect x="965" y="1079" width="261" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1038/s41467-018-08142-5" xlink:title="http://dx.doi.org/10.1038/s41467-018-08142-5"><rect x="965" y="1153" width="261" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05349"><rect x="878" y="1276" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1523/eneuro.0483-18.2019" xlink:title="http://dx.doi.org/10.1523/eneuro.0483-18.2019"><rect x="965" y="1269" width="261" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1038/s41586-019-1053-2" xlink:title="http://dx.doi.org/10.1038/s41586-019-1053-2"><rect x="965" y="1344" width="254" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05449"><rect x="878" y="1483" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.7554/eLife.18103" xlink:title="http://dx.doi.org/10.7554/eLife.18103"><rect x="965" y="1418" width="64" height="15"/></a>
<a xlink:href="https://aeon.co/ideas/how-your-mind-under-stress-gets-better-at-processing-bad-news" xlink:title="https://aeon.co/ideas/how-your-mind-under-stress-gets-better-at-processing-bad-news"><rect x="965" y="1491" width="250" height="15"/></a>
<a xlink:href="https://aeon.co/ideas/how-your-mind-under-stress-gets-better-at-processing-bad-news" xlink:title="https://aeon.co/ideas/how-your-mind-under-stress-gets-better-at-processing-bad-news"><rect x="965" y="1507" width="230" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05549"><rect x="878" y="1631" width="12" height="12"/></a>
<a xlink:href="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain" xlink:title="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain"><rect x="1114" y="1625" width="261" height="15"/></a>
<a xlink:href="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain" xlink:title="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain"><rect x="1114" y="1640" width="64" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05649"><rect x="878" y="1733" width="12" height="12"/></a>
<a xlink:href="http://doi.org/10.1073/pnas.1506552113" xlink:title="http://doi.org/10.1073/pnas.1506552113"><rect x="960" y="1780" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05749"><rect x="878" y="1877" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1126/science.aaf6005" xlink:title="http://dx.doi.org/10.1126/science.aaf6005"><rect x="1048" y="1864" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1126/science.aaf5656" xlink:title="http://dx.doi.org/10.1126/science.aaf5656"><rect x="1053" y="1927" width="89" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05849"><rect x="878" y="2012" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0956797616661182" xlink:title="http://dx.doi.org/10.1177/0956797616661182"><rect x="960" y="2051" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/05949"><rect x="878" y="2126" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.ssresearch.2016.08.007" xlink:title="http://dx.doi.org/10.1016/j.ssresearch.2016.08.007"><rect x="960" y="2165" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06049"><rect x="878" y="2204" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06149"><rect x="878" y="2275" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0956797616667721" xlink:title="http://dx.doi.org/10.1177/0956797616667721"><rect x="960" y="2306" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06249"><rect x="878" y="2374" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1177/1368430216677304" xlink:title="http://dx.doi.org/10.1177/1368430216677304"><rect x="960" y="2405" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06349"><rect x="878" y="2461" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1073/pnas.1617357114" xlink:title="http://dx.doi.org/10.1073/pnas.1617357114"><rect x="965" y="2469" width="210" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06449"><rect x="878" y="2534" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.jrp.2017.03.005" xlink:title="http://dx.doi.org/10.1016/j.jrp.2017.03.005"><rect x="965" y="2543" width="202" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06549"><rect x="878" y="2726" width="12" height="12"/></a>
<a xlink:href="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things" xlink:title="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things"><rect x="965" y="2675" width="266" height="15"/></a>
<a xlink:href="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things" xlink:title="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things"><rect x="965" y="2690" width="154" height="15"/></a>
<a xlink:href="http://theverge.com/2019/4/5/18297272/magic-psychology-optical-illusions-perception-cognition-experiencing-the-impossible" xlink:title="http://theverge.com/2019/4/5/18297272/magic-psychology-optical-illusions-perception-cognition-experiencing-the-impossible"><rect x="965" y="2763" width="266" height="31"/></a>
<a xlink:href="http://theverge.com/2019/4/5/18297272/magic-psychology-optical-illusions-perception-cognition-experiencing-the-impossible" xlink:title="http://theverge.com/2019/4/5/18297272/magic-psychology-optical-illusions-perception-cognition-experiencing-the-impossible"><rect x="965" y="2795" width="150" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06649"><rect x="878" y="2927" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1037/xge0000240" xlink:title="http://dx.doi.org/10.1037/xge0000240"><rect x="965" y="2943" width="69" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06749"><rect x="878" y="3412" width="12" height="12"/></a>
<a xlink:href="https://doi.org/10.1371/journal.pcbi.1005684" xlink:title="https://doi.org/10.1371/journal.pcbi.1005684"><rect x="965" y="3383" width="207" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1080/17524032.2018.1520735" xlink:title="http://dx.doi.org/10.1080/17524032.2018.1520735"><rect x="965" y="3808" width="266" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1080/17524032.2018.1520735" xlink:title="http://dx.doi.org/10.1080/17524032.2018.1520735"><rect x="965" y="3823" width="193" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06849"><rect x="878" y="3934" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1093/scan/nsy099" xlink:title="http://dx.doi.org/10.1093/scan/nsy099"><rect x="965" y="3898" width="211" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1038/s41598-017-14323-x" xlink:title="http://dx.doi.org/10.1038/s41598-017-14323-x"><rect x="965" y="3987" width="259" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/06949"><rect x="878" y="4061" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1037/pspi0000150" xlink:title="http://dx.doi.org/10.1037/pspi0000150"><rect x="965" y="4077" width="213" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07049"><rect x="878" y="4460" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1111/tops.12312" xlink:title="http://dx.doi.org/10.1111/tops.12312"><rect x="965" y="4787" width="204" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07149"><rect x="878" y="4860" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1037/pspi0000114" xlink:title="http://dx.doi.org/10.1037/pspi0000114"><rect x="965" y="4876" width="181" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07249"><rect x="878" y="4950" width="12" height="12"/></a>
<a xlink:href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth" xlink:title="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth"><rect x="965" y="4950" width="261" height="15"/></a>
<a xlink:href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth" xlink:title="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth"><rect x="965" y="4966" width="240" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07349"><rect x="878" y="5061" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0956797617743018" xlink:title="http://dx.doi.org/10.1177/0956797617743018"><rect x="965" y="5025" width="255" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1371/journal.pone.0204217" xlink:title="http://dx.doi.org/10.1371/journal.pone.0204217"><rect x="965" y="5099" width="260" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07449"><rect x="878" y="5217" width="12" height="12"/></a>
<a xlink:href="http://psycnet.apa.org/doi/10.1037/apl0000247" xlink:title="http://psycnet.apa.org/doi/10.1037/apl0000247"><rect x="965" y="5204" width="260" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.concog.2019.02.007" xlink:title="http://dx.doi.org/10.1016/j.concog.2019.02.007"><rect x="965" y="5278" width="262" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07549"><rect x="878" y="5343" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1177/1948550617741181" xlink:title="http://dx.doi.org/10.1177/1948550617741181"><rect x="965" y="5351" width="255" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07649"><rect x="878" y="5424" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1037/xge0000559" xlink:title="http://dx.doi.org/10.1037/xge0000559"><rect x="965" y="5440" width="210" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07749"><rect x="878" y="5506" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1126/science.aap8731" xlink:title="http://dx.doi.org/10.1126/science.aap8731"><rect x="965" y="5515" width="235" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07849"><rect x="878" y="5580" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.3390/e20120931" xlink:title="http://dx.doi.org/10.3390/e20120931"><rect x="965" y="5588" width="205" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/07949"><rect x="878" y="5667" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1038/s41598-019-42204-y" xlink:title="http://dx.doi.org/10.1038/s41598-019-42204-y"><rect x="965" y="5690" width="259" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08049"><rect x="878" y="5768" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1093/scan/nsz028" xlink:title="http://dx.doi.org/10.1093/scan/nsz028"><rect x="965" y="5791" width="211" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08149"><rect x="878" y="5837" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08249"><rect x="878" y="5874" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08349"><rect x="878" y="5911" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08449"><rect x="878" y="5947" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08549"><rect x="878" y="5984" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08649"><rect x="878" y="6021" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08749"><rect x="878" y="6151" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1177/0146167218779823" xlink:title="http://dx.doi.org/10.1177/0146167218779823"><rect x="965" y="6100" width="255" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08849"><rect x="878" y="6280" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/08949"><rect x="878" y="6317" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09049"><rect x="878" y="6409" width="12" height="12"/></a>
<a xlink:href="http://dx.doi.org/10.1016/j.neuron.2017.11.008" xlink:title="http://dx.doi.org/10.1016/j.neuron.2017.11.008"><rect x="965" y="6381" width="259" height="15"/></a>
<a xlink:href="http://dx.doi.org/10.1073/pnas.1801512115" xlink:title="http://dx.doi.org/10.1073/pnas.1801512115"><rect x="965" y="6454" width="242" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09149"><rect x="878" y="6587" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09249"><rect x="878" y="6710" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09349"><rect x="878" y="6774" width="12" height="12"/></a>
<a xlink:href="https://aeon.co/ideas/how-cotton-production-in-medieval-china-unravelled-patriarchy" xlink:title="https://aeon.co/ideas/how-cotton-production-in-medieval-china-unravelled-patriarchy"><rect x="965" y="6774" width="261" height="15"/></a>
<a xlink:href="https://aeon.co/ideas/how-cotton-production-in-medieval-china-unravelled-patriarchy" xlink:title="https://aeon.co/ideas/how-cotton-production-in-medieval-china-unravelled-patriarchy"><rect x="965" y="6790" width="194" height="15"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09449"><rect x="878" y="6837" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09549"><rect x="878" y="6874" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09649"><rect x="878" y="6911" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09749"><rect x="878" y="7520" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09849"><rect x="878" y="8130" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/09949"><rect x="878" y="8167" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04949"><rect x="746" y="5827" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04849"><rect x="746" y="5788" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04749"><rect x="746" y="5748" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04649"><rect x="746" y="5712" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04549"><rect x="746" y="5675" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04449"><rect x="746" y="5638" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04349"><rect x="746" y="5601" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04249"><rect x="746" y="5565" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04149"><rect x="746" y="5528" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/04049"><rect x="746" y="5491" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03949"><rect x="746" y="5454" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03849"><rect x="746" y="5417" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03749"><rect x="746" y="5381" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03649"><rect x="746" y="5344" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03549"><rect x="746" y="5307" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03449"><rect x="746" y="5270" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03349"><rect x="746" y="5234" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03249"><rect x="746" y="5197" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03149"><rect x="746" y="5160" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/03049"><rect x="746" y="5123" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02949"><rect x="746" y="5087" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02849"><rect x="746" y="5050" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02749"><rect x="746" y="5013" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02649"><rect x="746" y="4976" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02549"><rect x="746" y="4939" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02449"><rect x="746" y="4903" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02349"><rect x="746" y="4866" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02249"><rect x="746" y="4829" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02149"><rect x="746" y="4792" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/02049"><rect x="746" y="4756" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01949"><rect x="746" y="4719" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01849"><rect x="746" y="4682" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01749"><rect x="746" y="4645" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01649"><rect x="746" y="4608" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01549"><rect x="746" y="4572" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01449"><rect x="746" y="4535" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01349"><rect x="746" y="4498" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01249"><rect x="746" y="4461" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01149"><rect x="746" y="4425" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01049" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/01049"><rect x="746" y="4388" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00949" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00949"><rect x="746" y="4351" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00849" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00849"><rect x="746" y="4314" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00749" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00749"><rect x="746" y="4275" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00649" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00649"><rect x="746" y="4209" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00549" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00549"><rect x="746" y="4061" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00449" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00449"><rect x="746" y="3799" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00349" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00349"><rect x="746" y="3342" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00249" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00249"><rect x="746" y="2983" width="12" height="12"/></a>
<a xlink:href="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00149" xlink:title="file:///var/mobile/Library/Mobile%20Documents/iCloud~com~toketaware~ios~ithoughts/Documents/00149"><rect x="746" y="2696" width="12" height="12"/></a>
<a xlink:href="00049-3d640f3ee891be2476cc88a2b67409d5.html" xlink:title="00049-3d640f3ee891be2476cc88a2b67409d5.html"><rect x="746" y="2413" width="12" height="12"/></a>
</svg>
</div>
</div>

<br/>
<a href="javascript:showhide('outlineDiv')">Outline show/hide</a>
<div id="outlineDiv" style="display:none;">
<h1>00049</h1>
<p>candy</p>
<h2>05049</h2>
<ul>
<li>fallacy of obviousness
<ul>
<li>
<p><em>the fallacy of obviousness: a new interpretation of a classic psychology experiment will change your view of perception, judgment – even human nature</em><br />
teppo felin 2018<br />
<a href="https://aeon.co/essays/are-humans-really-blind-to-the-gorilla-on-the-basketball-court">https://aeon.co/essays/are-humans-really-blind-to-the-gorilla-on-the-basketball-court</a></p>
<ul>
<li>
<p>“computers and algorithms – even the most sophisticated ones – cannot address the fallacy of obviousness. Put differently, they can never know what might be relevant.”</p>
<p>“while Kahneman calls for large-scale replications of priming studies, the argument here is not that we need more studies or data to verify that people indeed miss blatantly obvious gorillas. Instead, we need better interpretation and better theories. After all, additional studies and data would undoubtedly verify the gorilla finding. But the more important issue is the interpretation of the gorilla finding and the experimental construction of the finding.</p>
<p>Having a ‘blind to the obvious’-baseline assumption about human nature biases the types of experiments that are crafted by scientists in the first place, what scientists go on to observe and look for, and how they interpret what they find. And importantly, the assumption of human blindness or bias makes scientists themselves blind to the other, more positive aspects of human cognition and nature. Thus the problem is more upstream, in the set of a priori questions and theories that science has. Namely, if our theory focuses on some aspect of human blindness and bias, and if we construct lab experiments to prove it (or look for naturally occurring instances of it), then yes, we are likely to find evidence.”</p>
<p>“Any well-intentioned efforts to correct human blindness also need to recognise that making these corrections comes with costs or tradeoffs. The trivial point here is that if we want to correct for the blindness of not spotting something (such as the gorilla), then this correction comes at the cost of attending to any number of other obvious things (eg, number of basketball passes). But the far more important point is that we also need to recognise and investigate the remarkable human capacities for generating questions and theories that direct our awareness and observations in the first place. Bias and blindness-obsessed studies will never get us to this vital recognition. In other words, continuing to construct experiments that look for and show bias and blindness – and adding them to the very large and growing list of cognitive biases and forms of blindness – will always leave out the remarkable capacities of humans to generate questions and theories. At its worst, the fascination with blindness and bias flattens humans, and science, to a morally dubious game of ‘gotcha’.”</p>
</li>
</ul>
</li>
<li>
<p><em>rationality, perception, and the all-seeing eye</em><br />
teppo felin et al. 2016<br />
<a href="https://doi.org/10.3758/s13423-016-1198-z">https://doi.org/10.3758/s13423-016-1198-z</a></p>
<ul>
<li>Seeing—perception and vision—is implicitly the fundamental building block of the literature on rationality and cognition. Herbert Simon and Daniel Kahneman’s arguments against the omniscience of economic agents—and the concept of bounded rationality—depend critically on a particular view of the nature of perception and vision. We propose that this framework of rationality merely replaces economic omniscience with perceptual omniscience. We show how the cognitive and social sciences feature a pervasive but problematic meta-assumption that is characterized by an “all-seeing eye.” We raise concerns about this assumption and discuss different ways in which the all-seeing eye manifests itself in existing research on (bounded) rationality. We first consider the centrality of vision and perception in Simon’s pioneering work. We then point to Kahneman’s work—particularly his article “Maps of Bounded Rationality”—to illustrate the pervasiveness of an all-seeing view of perception, as manifested in the extensive use of visual examples and illusions. Similar assumptions about perception can be found across a large literature in the cognitive sciences. The central problem is the present emphasis on inverse optics—the objective nature of objects and environments, e.g., size, contrast, and color. This framework ignores the nature of the organism and perceiver. We argue instead that reality is constructed and expressed, and we discuss the species-specificity of perception, as well as perception as a user interface. We draw on vision science as well as the arts to develop an alternative understanding of rationality in the cognitive and social sciences. We conclude with a discussion of the implications of our arguments for the rationality and decision-making literature in cognitive psychology and behavioral economics, along with suggesting some ways forward.</li>
</ul>
</li>
<li>
<p><em>the theory-based view: economic actors as theorists</em><br />
teppo felin and todd r. zenger 2017<br />
<a href="https://doi.org/10.1287/stsc.2017.0048">https://doi.org/10.1287/stsc.2017.0048</a></p>
<ul>
<li>This paper outlines the theory-based view of strategy and markets. We argue that novel or “great” strategies come from theories. Entrepreneurs and managers originate theories and hypotheses about which activities they should engage in, which assets they might buy, and how they will create value. A firm’s strategy, then, represents a set of contrarian beliefs and a theory—a unique, firm-specific point of view—about what problems to solve, and how to organize and govern the overall process of value creation. We outline the cognitive and perceptual, organizational, and economic foundations of the theory-based view of strategy. We also discuss the essential attributes needed for a firm-level theory of strategy. Throughout the paper we offer informal examples of our argument, by briefly discussing the strategies of companies like Apple, Uber, Disney, Wal-Mart, and Airbnb. The theory-based view of strategy and markets also offers important insights for how firms govern themselves (including ownership, boards, and organization design) and how firms interact with capital markets and external evaluators and stakeholders. We conclude with a discussion of the practical and managerial applications of the theory-based view.</li>
</ul>
</li>
<li>
<p><em>mind, rationality, and cognition: an interdisciplinary debate</em><br />
nick chater et al. 2017<br />
<a href="https://doi.org/10.3758/s13423-017-1333-5">https://doi.org/10.3758/s13423-017-1333-5</a></p>
<ul>
<li>This article features an interdisciplinary debate and dialogue about the nature of mind, perception, and rationality. Scholars from a range of disciplines—cognitive science, applied and experimental psychology, behavioral economics, and biology—offer critiques and commentaries of a target article by Felin, Koenderink, and Krueger (<u>2017</u>): “Rationality, Perception, and the All-Seeing Eye,” <em>Psychonomic Bulletin &amp; Review</em>. The commentaries raise a number of criticisms and issues concerning rationality and the all-seeing-eye argument, including the nature of judgment and reasoning, biases versus heuristics, organism–environment relations, perception and situational construal, equilibrium analysis in economics, efficient markets, and the nature of empirical observation and the scientific method. The debated topics have far-reaching consequences for the rationality literature specifically, as well as for the cognitive, psychological, and economic sciences more broadly. The commentaries are followed by a response from the authors of the target article. Their response is organized around three central issues: (1) the problem of cues; (2) what is the question?; and (3) equilibria, $500 bills, and the axioms of rationality.</li>
</ul>
</li>
</ul>
</li>
<li>punctuated attention
<ul>
<li>
<p><em>scientists reveal the number of times you’re actually conscious each minute: spoiler: it’s not very often (and that’s a good thing)</em><br />
emma betuel 2018<br />
<a href="https://www.inverse.com/article/48300-why-is-it-hard-to-focus-research-humans">https://www.inverse.com/article/48300-why-is-it-hard-to-focus-research-humans</a></p>
<ul>
<li>
<p>Four times every second, explains Princeton Neuroscience Institute Ian Fiebelkorn, Ph.D., to Inverse, the brain stops focusing on the task at hand. That’s about 240 times a minute.</p>
<p>“The brain is wired to be somewhat distractible.”</p>
<p>“The brain is wired to be somewhat distractible,” he says. “We focus in bursts, and between those bursts we have these periods of distractibility, that’s when the brain seems to check in on the rest of the environment outside to see if there’s something important going on elsewhere. These rhythms are affecting our behavior all the time.”<br />
To understand these “rhythms of attention,” Fiebelkorn suggests imagining standing in Times Square on New Years’ Eve, surrounded by people, cars, and music. The scene presents far more sensory information than one human brain is capable of sorting through, and so, the brain deals with all of the information in two ways. First, it focuses on a single point of interest: the street corner where you might meet a friend, or Ryan Seacrest combing the crowd for interviews. Like a filmstrip, the brain takes snapshots of these moments and pieces them together into a cohesive narrative, or “perceptual cycle.”</p>
<p>We experience that moment as continuous, but in reality, we’ve only sampled certain elements of the environment around us. It feels continuous because our brains have filled in the gaps for us, explains Berkeley’s Knight Lab researcher and first author Randolph Helfrich, Ph.D. to Inverse.</p>
<p>“I think it’s more a philosophical problem that it is a scientific problem,” he says. “Because when we look at brain data we see a pattern that waxes and wanes, they’re never constant and stable. Everyone perceives the world as continuous and coherent, but the real tricky part is, how does the brain do that?”</p>
<p>Modern society tends to think of distractibility as a bad thing, Fiebelkorn says, but it might have offered early humans and our distant ancestors a huge evolutionary advantage. The brain’s natural tendency to “zoom out” and become distracted by the environment, even for just a few milliseconds, could have allowed them the time to detect the presence of a threat and react accordingly.</p>
<p>“Say you spot a shiny red apple in a tree and you focus on that,” Fiebelkorn says. “You’re going to go and pick that apple, but you’ll want to know if there’s any larger animal with bigger teeth going after the same apple. So, having these windows of distractibility helps you to detect these stimuli you might otherwise miss.”</p>
<p>The findings of these two papers in conjunction are powerful evidence that these rhythms are highly adaptive and have been preserved in humans and their relative species for millions of years. This hypothesis is based on the fact that the teams noticed nearly identical neural patterns of attention (the “rhythms of distractability”) in both the humans and macaques. For a trait to still be so similar in species that diverged from a common ancestor billions of years ago, it very likely must provide a useful evolutionary advantage that has been preserved by natural selection.</p>
</li>
</ul>
</li>
<li>
<p><em>neural mechanisms of sustained attention are rhythmic</em><br />
randolph f. helfrich et al. 2018<br />
<a href="https://doi.org/10.1016/j.neuron.2018.07.032">https://doi.org/10.1016/j.neuron.2018.07.032</a></p>
<ul>
<li>
<p>“Our subjective experience of the visual world is an illusion,” said Sabine Kastner, a professor of psychology and the Princeton Neuroscience Institute (PNI). “Perception is discontinuous, going rhythmically through short time windows when we can perceive more or less.”<br />
The researchers use different metaphors to describe this throb of attention, including a spotlight that waxes and wanes in its intensity. Four times per second — once every 250 milliseconds — the spotlight dims and the house lights come up. Instead of focusing on the action “onstage,” your brain takes in everything else around you, say the scientists.<br />
Their work appears as a set of back-to-back papers in in the Aug. 22 issue of <em>Neuron</em>; one paper focuses on human research subjects, the other on macaque monkeys.<br />
“The question is: How can something that varies in time support our seemingly continuous perception of the world?” said Berkeley’s Randolph Helfrich, first author on the human-focused paper. “There are only two options: Is the data wrong, or is our understanding of our perception biased? Our research shows that it’s the latter. Our brains fuse our perceptions into a coherent movie — we don’t experience the gaps.”<br />
Perception doesn’t flicker on and off, the researchers emphasized, but four times per second it cycles between periods of maximum focus and periods of a broader situational awareness.<br />
“Every 250 milliseconds, you have an opportunity to switch attention,” said Ian Fiebelkorn, an associate research scholar in PNI and the first author on the macaque-focused paper. You won’t necessarily shift your focus to a new subject, he said, but your brain has a chance to re-examine your priorities and decide if it wants to.<br />
Brain rhythms have been known for almost a century, since electroencephalograms — better known as EEGs — were invented in 1924. “But we didn’t really understand what these rhythms are for,” said Kastner, who was the senior author on both papers. “We can now link brain rhythms for the first time to our behavior, on a moment-to-moment basis. ... This is a very surprising finding, more since these rhythmic processes are evolutionarily old — we find them in non-human primates as well as in our own species.”<br />
This pulsing attention must present an evolutionary advantage, the researchers suggest, perhaps because focusing too intently on one subject could allow a threat to catch us by surprise.<br />
“Attention is fluid, and you want it to be fluid,” said Fiebelkorn. “You don’t want to be over-locked on anything. It seems like it’s an evolutionary advantage to have these windows of opportunity where you’re checking in with your environment.”<br />
“It’s an elegant way to allocate brain resources — to sample the environment and not have any lapses,” said Robert Knight, a professor of psychology and neuroscience at Berkeley and a co-author on the human-focused paper.<br />
Kastner’s lab focuses on macaque research, so she reached out to Knight’s lab, which does similar studies on humans. The resulting papers are unprecedented, Knight said.<br />
“This is cross-species validation of a fundamental aspect of human behavior,” he said. “I have not seen any back-to-back human and monkey papers appear anywhere ... and these are in the same issue of <em>Neuron</em>, a preeminent journal.”<br />
Fiebelkorn agreed: “We have an assumption that what we find in the monkey will hold up in humans, but it’s rarely checked as carefully as it is here.”<br />
“Originally, we wanted to study something very different,” said Kastner. “We wanted to ask how we can select objects from our cluttered visual environments. ... We were particularly looking at how the intake of visual information unfolds over time — something that is rarely done in behavioral studies — and this revealed the rhythmic structure of perception. It was a complete surprise finding.”</p>
</li>
<li>
<p><em>abstract</em> The functional architecture of attention is rhythmic<br />
Frontoparietal theta activity predicts behavior on a rapid timescale<br />
Theta activity controls cortical excitability and information flow<br />
Rhythmic sampling is independent of task structure and context</p>
<p>Classic models of attention suggest that sustained neural firing constitutes a neural correlate of sustained attention. However, recent evidence indicates that behavioral performance fluctuates over time, exhibiting temporal dynamics that closely resemble the spectral features of ongoing, oscillatory brain activity. Therefore, it has been proposed that periodic neuronal excitability fluctuations might shape attentional allocation and overt behavior. However, empirical evidence to support this notion is sparse. Here, we address this issue by examining data from large-scale subdural recordings, using two different attention tasks that track perceptual ability at high temporal resolution. Our results reveal that perceptual outcome varies as a function of the theta phase even in states of sustained spatial attention. These effects were robust at the single-subject level, suggesting that rhythmic perceptual sampling is an inherent property of the frontoparietal attention network. Collectively, these findings support the notion that the functional architecture of top-down attention is intrinsically rhythmic.</p>
</li>
</ul>
</li>
<li>
<p><em>a dynamic interplay within the frontoparietal network underlies rhythmic spatial attention</em><br />
ian c. fiebelkorn et al. 2018<br />
<a href="https://doi.org/10.1016/j.neuron.2018.07.038">https://doi.org/10.1016/j.neuron.2018.07.038</a></p>
<ul>
<li>
<p><em>abstract</em> Non-human primates, like humans, sample the visual scene in rhythmic cycles<br />
Neural oscillations in the frontoparietal network modulate perceptual sensitivity<br />
Theta phase acts as a clocking mechanism, organizing alternating attentional states<br />
Temporal dynamics linked to specific function and cell type define attentional state</p>
<p>Classic studies of spatial attention assumed that its neural and behavioral effects were continuous over time. Recent behavioral studies have instead revealed that spatial attention leads to alternating periods of heightened or diminished perceptual sensitivity. Yet, the neural basis of these rhythmic fluctuations has remained largely unknown. We show that a dynamic interplay within the macaque frontoparietal network accounts for the rhythmic properties of spatial attention. Neural oscillations characterize functional interactions between the frontal eye fields (FEF) and the lateral intraparietal area (LIP), with theta phase (3–8 Hz) coordinating two rhythmically alternating states. The first is defined by FEF-dominated beta-band activity, associated with suppressed attentional shifts, and LIP-dominated gamma-band activity, associated with enhanced visual processing and better behavioral performance. The second is defined by LIP-specific alpha-band activity, associated with attenuated visual processing and worse behavioral performance. Our findings reveal how network-level interactions organize environmental sampling into rhythmic cycles.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>05149</h2>
<ul>
<li>
<p>neuroscientist lisa feldman barrett explains how emotions are made 2017<br />
<a href="http://www.theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett">theverge.com/2017/4/10/15245690/how-emotions-are-made-neuroscience-lisa-feldman-barrett</a></p>
<ul>
<li>“I think understanding how emotions are constructed widens the horizon of control. You realize that if your brain is using your past to construct your present, you can invest energy in the present to cultivate new experiences that then become the seeds for your future. You can cultivate or curate experiences in the now and then they become, if you practice them, they become automated enough that your brain will automatically construct them in the future.”</li>
</ul>
</li>
</ul>
<h2>05249</h2>
<ul>
<li>
<p><em>the vision thing: how babies colour in the world</em><br />
nicola davis 2017<br />
<a href="https://www.theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world?CMP=share_btn_fb">theguardian.com/lifeandstyle/2017/apr/11/vision-thing-how-babies-colour-in-the-world</a></p>
</li>
<li>
<p><em>the sea was never blue</em><br />
the greek colour experience was made of movement and shimmer. can we ever glimpse what they saw when gazing out to sea?<br />
maria michela sassi 2017<br />
<a href="https://aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world">aeon.co/essays/can-we-hope-to-understand-how-the-greeks-saw-their-world</a></p>
</li>
<li>
<p><em>hunter-gatherer olfaction is special</em><br />
majid and kruspe 2018<br />
<a href="http://dx.doi.org/10.1016/j.cub.2017.12.014">http://dx.doi.org/10.1016/j.cub.2017.12.014</a></p>
<ul>
<li>
<p><em>abstract</em> •People struggle to name odors, but this limitation is not universal<br />
•Is superior olfactory performance due to subsistence, ecology or language family?<br />
•Hunter-gatherers and non-hunter-gatherers from the same environment were compared<br />
•Only hunter-gatherers were proficient odor namers, showing subsistence is crucial</p>
<p>People struggle to name odors. This has been attributed to a diminution of olfaction in trade-off to vision. This presumption has been challenged recently by data from the hunter-gatherer Jahai who, unlike English speakers, find odors as easy to name as colors. Is the superior olfactory performance among the Jahai because of their ecology (tropical rainforest), their language family (Aslian), or because of their subsistence (they are hunter-gatherers)? We provide novel evidence from the hunter-gatherer Semaq Beri and the non-hunter-gatherer (swidden-horticulturalist) Semelai that subsistence is the critical factor. Semaq Beri and Semelai speakers—who speak closely related languages and live in the tropical rainforest of the Malay Peninsula—took part in a controlled odor- and color-naming experiment. The swidden-horticulturalist Semelai found odors much more difficult to name than colors, replicating the typical Western finding. But for the hunter-gatherer Semaq Beri odor naming was as easy as color naming, suggesting that hunter-gatherer olfactory cognition is special.</p>
</li>
<li>
<p>“There has been a long-standing consensus that ‘smell is the mute sense, the one without words,’ and decades of research with English-speaking participants seemed to confirm this,” says Asifa Majid of Radboud University in the Netherlands. “But, the Jahai of the Malay Peninsula are much better at naming odors than their English-speaking peers. This, of course, raises the question of where this difference originates.”<br />
To find out whether it was the Jahai who have an unusually keen ability with odors or whether English speakers are simply lacking, Majid and Nicole Kruspe at Lund University in Sweden looked to two related, but previously unstudied, groups of people in the tropical rainforest of the Malay Peninsula: the hunter-gatherer Semaq Beri and the non-hunter-gatherer Semelai. The Semelai are traditionally horticulturalists, combining shifting rice cultivation with the collection of forest products for trade.<br />
The Semaq Beri and Semelai not only live in a similar environment; they also speak closely related languages. The question was: how were they at naming odors?<br />
“If ease of olfactory naming is related to cultural practices, then we would expect the Semaq Beri to behave like the Jahai and name odors as easily as they do colors, whereas the Semelai should pattern differently,” the researchers wrote. And, that’s exactly what they found.<br />
Majid and Kruspe tested the color- and odor-naming abilities of 20 Semaq Beri and 21 Semelai people. Sixteen odors were used: orange, leather, cinnamon, peppermint, banana, lemon, licorice, turpentine, garlic, coffee, apple, clove, pineapple, rose, anise, and fish. For the color task, study participants saw 80 Munsell color chips, sampling 20 equally spaced hues at four degrees of brightness. Kruspe tested participants in their native language by simply asking, “What smell is this?” or “What color is this?”<br />
The results were clear. The hunter-gatherer Semaq Beri performed on those tests just like the hunter-gatherer Jahai, naming odors and colors with equal ease. The non-hunter-gatherer Semelai, on the other hand, performed like English speakers. For them, odors were difficult to name.<br />
The results suggest that the downgrading in importance of smells relative to other sensory inputs is a recent consequence of cultural adaption, the researchers say. “Hunter-gatherers’ olfaction is superior, while settled peoples’ olfactory cognition is diminished,” Majid says.<br />
They say the findings challenge the notion that differences in neuroarchitecture alone underlie differences in olfaction, suggesting instead that cultural variation may play a more prominent role. They also raise a number of interesting questions: “Do hunter-gatherers in other parts of the world also show the same boost to olfactory naming?” Majid asks. “Are other aspects of olfactory cognition also superior in hunter-gatherers,” for example, the ability to differentiate one odor from another? “Finally, how do these cultural differences interact with the biological infrastructure for smell?” She says it will be important to learn whether these groups of people show underlying genetic differences related to the sense of smell.</p>
</li>
</ul>
</li>
<li>
<p><em>spatial representations of the viewer’s surroundings</em><br />
satoshi shioiri et al. 2018<br />
<a href="http://dx.doi.org/10.1038/s41598-018-25433-5">http://dx.doi.org/10.1038/s41598-018-25433-5</a></p>
<ul>
<li>
<p>Spatial representation surrounding a viewer including outside the visual field is crucial for moving around the three-dimensional world. To obtain such spatial representations, we predict that there is a learning process that integrates visual inputs from different viewpoints covering all the 360° visual angles. We report here the learning effect of the spatial layouts on six displays arranged to surround the viewer, showing shortening of visual search time on surrounding layouts that are repeatedly used (contextual cueing effect). The learning effect is found even in the time to reach the display with the target as well as the time to reach the target within the target display, which indicates that there is an implicit learning effect on spatial configurations of stimulus elements across displays. Since, furthermore, the learning effect is found between layouts and the target presented on displays located even 120° apart, this effect should be based on the representation that covers visual information far outside the visual field.</p>
</li>
<li>
<p>contextual cueing effect (CCE),<br />
CCE of surrounds (CCES)</p>
<p>the visual system constructs representations that link information within the visual field and information outside the visual field through repeated observation of the same spatial arrangements, that is, the CCES. The CCES is implicit and done without awareness of repeated observation of the same stimulus. Representations obtained by repetition without awareness are useful for moving around in familiar spaces and also in spaces that have structures in common with familiar places. Such representations should support actions in everyday life as well as specific actions for sports, driving, and so on.</p>
</li>
</ul>
</li>
<li>
<p><em>avian uv vision enhances leaf surface contrasts in forest environments</em><br />
cynthia tedore, dan-eric nilsson 2019<br />
<a href="http://dx.doi.org/10.1038/s41467-018-08142-5">http://dx.doi.org/10.1038/s41467-018-08142-5</a></p>
<ul>
<li>
<p>Human colour vision is based on three primary colours: red, green and blue. The colour vision of birds is based on the same three colours -- but also ultraviolet. Biologists at Lund have now shown that the fourth primary colour of birds, ultraviolet, means that they see the world in a completely different way. Among other things, birds see contrasts in dense forest foliage, whereas people only see a wall of green.</p>
<p>&quot;What appears to be a green mess to humans are clearly distinguishable leaves for birds. No one knew about this until this study,&quot; says Dan-Eric Nilsson, professor at the Department of Biology at Lund University.</p>
<p>For birds, the upper sides of leaves appear much lighter in ultraviolet. From below, the leaves are very dark. In this way the three-dimensional structure of dense foliage is obvious to birds. This in turn makes it easy for them to move, find food and navigate. People, on the other hand, do not perceive ultraviolet, and see the foliage in green; the primary colour where contrast is the worst.</p>
<p>Dan-Eric Nilsson founded the world-leading Lund Vision Group at Lund University. The study in question is a collaboration with Cynthia Tedore and was conducted during her time as a postdoc in Lund. She is now working at the University of Hamburg.</p>
<p>It is the first time that researchers have succeeded in imitating bird colour vision with a high degree of precision. This was achieved with the help of a unique camera and advanced calculations. The camera was designed within the Lund Vision Group and equipped with rotating filter wheels and specially manufactured filters, which make it possible to show what different animals see clearly. In this case, the camera imitates with a high degree of accuracy the colour sensitivity of the four different types of cones in bird retinas.</p>
<p>&quot;We have discovered something that is probably very important for birds, and we continue to reveal how reality appears also to other animals,&quot; says Dan-Eric Nilsson, continuing:</p>
<p>&quot;We may have the notion that what we see is the reality, but it's a highly human reality. Other animals live in other realities, and we can now see through their eyes and reveal many secrets. Reality is in the eye of the beholder,&quot; he concludes.</p>
</li>
<li>
<p><em>abstract</em> UV vision is prevalent, but we know little about its utility in common general tasks, as in resolving habitat structure. Here we visualize vegetated habitats using a multispectral camera with channels mimicking bird photoreceptor sensitivities across the UV-visible spectrum. We find that the contrast between upper and lower leaf surfaces is higher in a UV channel than in any visible channel, and that this makes leaf position and orientation stand out clearly. This was unexpected since both leaf surfaces reflect similarly small proportions (1–2%) of incident UV light. The strong UV-contrast can be explained by downwelling light being brighter than upwelling, and leaves transmitting &lt; 0.06% of incident UV light. We also find that mirror-like specular reflections of the sky and overlying canopy, from the waxy leaf cuticle, often dwarf diffuse reflections. Specular reflections shift leaf color, such that maximum leaf-contrast is seen at short UV wavelengths under open canopies, and at long UV wavelengths under closed canopies.</p>
</li>
</ul>
</li>
</ul>
<h2>05349</h2>
<ul>
<li>
<p>human senses</p>
</li>
<li>
<p><em>transduction of the geomagnetic field as evidenced from alpha-band activity in the human brain</em><br />
connie x. wang et al. 2019<br />
<a href="http://dx.doi.org/10.1523/eneuro.0483-18.2019">http://dx.doi.org/10.1523/eneuro.0483-18.2019</a></p>
<ul>
<li>
<p>Many animals, such as migratory birds and sea turtles, have a geomagnetic sense that supports their biological navigation system. Although magnetoreception has been well-studied in these animals, scientists have not yet been able to determine whether humans share this ability.</p>
<p>Geoscientist Joseph Kirschvink, neuroscientist Shin Shimojo, and their colleagues at Caltech and the University of Tokyo set out to address this long-standing question using electroencephalography to record adult participants' brain activity during magnetic field manipulations. Carefully controlled experiments revealed a decrease in alpha-band brain activity -- an established response to sensory input -- in some participants. The researchers replicated this effect in participants who responded strongly and confirmed these responses were tuned to the magnetic field of the Northern Hemisphere, where the study was conducted.</p>
<p>Future studies of magnetoreception in diverse human populations may provide new clues into the evolution and individual variation of this ancient sensory system.</p>
</li>
<li>
<p><em>abstract</em> Magnetoreception, the perception of the geomagnetic field, is a sensory modality well-established across all major groups of vertebrates and some invertebrates, but its presence in humans has been tested rarely, yielding inconclusive results. We report here a strong, specific human brain response to ecologically-relevant rotations of Earth-strength magnetic fields. Following geomagnetic stimulation, a drop in amplitude of EEG alpha oscillations (8-13 Hz) occurred in a repeatable manner. Termed alpha event-related desynchronization (alpha-ERD), such a response has been associated previously with sensory and cognitive processing of external stimuli including vision, auditory and somatosensory cues. Alpha-ERD in response to the geomagnetic field was triggered only by horizontal rotations when the static vertical magnetic field was directed downwards, as it is in the Northern Hemisphere; no brain responses were elicited by the same horizontal rotations when the static vertical component was directed upwards. This implicates a biological response tuned to the ecology of the local human population, rather than a generic physical effect.</p>
<p>Biophysical tests showed that the neural response was sensitive to static components of the magnetic field. This rules out all forms of electrical induction (including artifacts from the electrodes) which are determined solely on dynamic components of the field. The neural response was also sensitive to the polarity of the magnetic field. This rules out free-radical 'quantum compass' mechanisms like the cryptochrome hypothesis, which can detect only axial alignment. Ferromagnetism remains a viable biophysical mechanism for sensory transduction and provides a basis to start the behavioral exploration of human magnetoreception.</p>
<p>Significance Statement Although many migrating and homing animals are sensitive to Earth’s magnetic field, most humans are not consciously aware of the geomagnetic stimuli that we encounter in everyday life. Either we have lost a shared, ancestral magnetosensory system, or the system lacks a conscious component with detectable neural activity but no apparent perceptual awareness by us. We found two classes of ecologically-relevant rotations of Earth-strength magnetic fields that produce strong, specific and repeatable effects on human brainwave activity in the EEG alpha band (8-13 Hz); EEG discriminates in response to different geomagnetic field stimuli. Biophysical tests rule out all except the presence of a ferromagnetic transduction element, such as biologically-precipitated crystals of magnetite (Fe3O4).</p>
</li>
</ul>
</li>
<li>
<p><em>chemosensory modulation of neural circuits for sodium appetite</em><br />
sangjun lee et al. 2019<br />
<a href="http://dx.doi.org/10.1038/s41586-019-1053-2">http://dx.doi.org/10.1038/s41586-019-1053-2</a></p>
<ul>
<li>
<p>When the body is low on sodium, the brain triggers specific appetite signals that drive the consumption of sodium. Though the mechanisms of these appetite signals are not fully understood, a team of researchers has now discovered a small population of neurons in the mouse hindbrain that controls the drive to consume sodium.</p>
<p>Led by graduate student Sangjun Lee, the team used genetic tools to manipulate the activity of these neurons so that they could be stimulated with light. The researchers observed that artificially stimulating these neurons caused mice to lick a piece of rock salt repeatedly, even when their bodies were completely sated with sodium.</p>
<p>Next, the researchers measured the activity of these neurons while mice ate sodium. Within several seconds of sodium hitting the animal's tongue, the activity of the sodium-appetite neurons was inhibited. However, a direct infusion of sodium into the stomach of these mice did not suppress the neural activity. This neural suppression also did not occur when sodium receptors on the tongue were pharmacologically blocked. Taken together, the research shows that oral sodium signals, likely mediated by the taste system, are necessary to inhibit the sodium-appetite neurons.</p>
<p>&quot;The desire to eat salt is the body's way of telling you that your body is low on sodium,&quot; says Oka. &quot;Once sodium is consumed, it takes some time for the body to fully absorb it. So, it's interesting that just the taste of sodium is sufficient to quiet down the activity of the salt-appetite neurons, which means that sensory systems like taste are much more important in regulating the body's functions than simply conveying external information to the brain.&quot;</p>
<p>Interestingly, in many species, including humans, consuming sodium can drive the desire to eat even more. In future work, Oka and his collaborators would like to understand how sodium-appetite neurons are modulated over time. Answering this question may open up avenues to help people with health issues to eat less sodium in their diets.</p>
</li>
<li>
<p><em>abstract</em> Sodium is the main cation in the extracellular fluid and it regulates various physiological functions. Depletion of sodium in the body increases the hedonic value of sodium taste, which drives animals towards sodium consumption1,2. By contrast, oral sodium detection rapidly quenches sodium appetite3,4, suggesting that taste signals have a central role in sodium appetite and its satiation. Nevertheless, the neural mechanisms of chemosensory-based appetite regulation remain poorly understood. Here we identify genetically defined neural circuits in mice that control sodium intake by integrating chemosensory and internal depletion signals. We show that a subset of excitatory neurons in the pre-locus coeruleus express prodynorphin, and that these neurons are a critical neural substrate for sodium-intake behaviour. Acute stimulation of this population triggered robust ingestion of sodium even from rock salt, while evoking aversive signals. Inhibition of the same neurons reduced sodium consumption selectively. We further demonstrate that the oral detection of sodium rapidly suppresses these sodium-appetite neurons. Simultaneous in vivo optical recording and gastric infusion revealed that sodium taste—but not sodium ingestion per se—is required for the acute modulation of neurons in the pre-locus coeruleus that express prodynorphin, and for satiation of sodium appetite. Moreover, retrograde-virus tracing showed that sensory modulation is in part mediated by specific GABA (γ-aminobutyric acid)-producing neurons in the bed nucleus of the stria terminalis. This inhibitory neural population is activated by sodium ingestion, and sends rapid inhibitory signals to sodium-appetite neurons. Together, this study reveals a neural architecture that integrates chemosensory signals and the internal need to maintain sodium balance.</p>
</li>
</ul>
</li>
</ul>
<h2>05449</h2>
<ul>
<li>
<p><em>unexpected arousal modulates the influence of sensory noise on confidence</em><br />
micah allen et al. 2016<br />
<a href="http://dx.doi.org/10.7554/eLife.18103">eLife.18103</a></p>
<ul>
<li>“relevant to understanding clinical disorders, such as anxiety and depression, where changes in arousal might lock sufferers into an unrealistically certain or uncertain world”</li>
<li>Human perception is invariably accompanied by a graded feeling of confidence that guides metacognitive awareness and decision-making. It is often assumed that this arises solely from the feed-forward encoding of the strength or precision of sensory inputs. In contrast, interoceptive inference models suggest that confidence reflects a weighted integration of sensory precision and expectations about internal states, such as arousal. Here we test this hypothesis using a novel psychophysical paradigm, in which unseen disgust-cues induced unexpected, unconscious arousal just before participants discriminated motion signals of variable precision. Across measures of perceptual bias, uncertainty, and physiological arousal we found that arousing disgust cues modulated the encoding of sensory noise. Furthermore, the degree to which trial-by-trial pupil fluctuations encoded this nonlinear interaction correlated with trial level confidence. Our results suggest that unexpected arousal regulates perceptual precision, such that subjective confidence reflects the integration of both external sensory and internal, embodied states.</li>
</ul>
</li>
<li>
<p><em>how your mind, under stress, gets better at processing bad news</em><br />
tali sharot 2018<br />
<a href="https://aeon.co/ideas/how-your-mind-under-stress-gets-better-at-processing-bad-news">https://aeon.co/ideas/how-your-mind-under-stress-gets-better-at-processing-bad-news</a></p>
</li>
<li>
<p><em>the influential mind: what the brain reveals about our power to change others</em><br />
tali sharot 2017</p>
</li>
</ul>
<h2>05549</h2>
<ul>
<li>supernormal stimuli
<ul>
<li><em>pikachu is a chocolate milkshake</em><br />
joel frohlich 2016<br />
<a href="https://aeon.co/ideas/how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain">how-the-cute-pikachu-is-a-chocolate-milkshake-for-the-brain</a></li>
</ul>
</li>
</ul>
<h2>05649</h2>
<ul>
<li>
<p><em>covert digital manipulation of vocal emotion alter speakers’ emotional states in a congruent direction</em><br />
jean-julien aucouturier, petter johansson, lars hall, rodrigo segnini, lolita mercadié, and katsumi watanabe 2016<br />
http://doi.org/10.1073/pnas.1506552113<br />
Link: <a href="http://doi.org/10.1073/pnas.1506552113">doi.org/10.1073/pnas.1506552113</a></p>
<ul>
<li>
<p><em>Significance</em><br />
We created a digital audio platform to covertly modify the emotional tone of participants’ voices while they talked toward happiness, sadness, or fear. Independent listeners perceived the transformations as natural examples of emotional speech, but the participants remained unaware of the manipulation, indicating that we are not continuously monitoring our own emotional signals. Instead, as a consequence of listening to their altered voices, the emotional state of the participants changed in congruence with the emotion portrayed. This result is the first evidence, to our knowledge, of peripheral feedback on emotional experience in the auditory domain. This finding is of great significance, because the mechanisms behind the production of vocal emotion are virtually unknown.</p>
</li>
<li>
<p><em>Abstract</em><br />
Research has shown that people often exert control over their emotions. By modulating expressions, reappraising feelings, and redirecting attention, they can regulate their emotional experience. These findings have contributed to a blurring of the traditional boundaries between cognitive and emotional processes, and it has been suggested that emotional signals are produced in a goal-directed way and monitored for errors like other intentional actions. However, this interesting possibility has never been experimentally tested. To this end, we created a digital audio platform to covertly modify the emotional tone of participants’ voices while they talked in the direction of happiness, sadness, or fear. The result showed that the audio transformations were being perceived as natural examples of the intended emotions, but the great majority of the participants, nevertheless, remained unaware that their own voices were being manipulated. This finding indicates that people are not continuously monitoring their own voice to make sure that it meets a predetermined emotional target. Instead, as a consequence of listening to their altered voices, the emotional state of the participants changed in congruence with the emotion portrayed, which was measured by both self-report and skin conductance level. This change is the first evidence, to our knowledge, of peripheral feedback effects on emotional experience in the auditory domain. As such, our result reinforces the wider framework of self-perception theory: that we often use the same inferential strategies to understand ourselves as those that we use to understand others.</p>
</li>
</ul>
</li>
</ul>
<h2>05749</h2>
<ul>
<li>sensing
<ul>
<li>
<p><em>phytochromes function as thermosensors in arabidopsis</em><br />
jae-hoon jung et al. 2016<br />
http://dx.doi.org/10.1126/science.aaf6005<br />
Link: <a href="http://dx.doi.org/10.1126/science.aaf6005">dx.doi.org/10.1126/science.aaf6005</a></p>
<ul>
<li>Plants are responsive to temperature, and can distinguish differences of 1°C. In <em>Arabidopsis</em>, warmer temperature accelerates flowering and increases elongation growth (thermomorphogenesis). The mechanisms of temperature perception are however largely unknown. We describe a major thermosensory role for the phytochromes (red light receptors) during the night. Phytochrome null plants display a constitutive warm temperature response, and consistent with this, we show in this background that the warm temperature transcriptome becomes de-repressed at low temperatures. We have discovered phytochrome B (phyB) directly associates with the promoters of key target genes in a temperature dependent manner. The rate of phyB inactivation is proportional to temperature in the dark, enabling phytochromes to function as thermal timers, integrating temperature information over the course of the night.</li>
</ul>
</li>
<li>
<p><em>phytochrome b integrates light and temperature signals in arabidopsis</em><br />
martina legris et al. 2016<br />
<a href="http://dx.doi.org/10.1126/science.aaf5656">science.aaf5656</a></p>
<ul>
<li>Ambient temperature regulates many aspects of plant growth and development but its sensors are unknown. Here, we demonstrate that the phytochrome B (phyB) photoreceptor participates in temperature perception through its temperature-dependent reversion from the active Pfr state to the inactive Pr state. Increased rates of thermal reversion upon exposing <em>Arabidopsis</em> seedlings to warm environments reduce both the abundance of the biologically active Pfr-Pfr dimer pool of phyB and the size of the associated nuclear bodies, even in daylight. Mathematical analysis of stem growth for seedlings expressing wild-type phyB or thermally stable variants under various combinations of light and temperature revealed that phyB is physiologically responsive to both signals. We therefore propose that in addition to its photoreceptor functions, phyB is a temperature sensor in plants.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>05849</h2>
<ul>
<li>
<p><em>young children see a single action and infer a social norm: promiscuous normativity in 3-year-olds</em><br />
marco f. h. schmidt, lucas p. butler, julia heinz, and michael tomasello 2016<br />
http://dx.doi.org/10.1177/0956797616661182<br />
Link: <a href="http://dx.doi.org/10.1177/0956797616661182">dx.doi.org/10.1177/0956797616661182</a></p>
<ul>
<li>Human social life depends heavily on social norms that prescribe and proscribe specific actions. Typically, young children learn social norms from adult instruction. In the work reported here, we showed that this is not the whole story: Three-year-old children are promiscuous normativists. In other words, they spontaneously inferred the presence of social norms even when an adult had done nothing to indicate such a norm in either language or behavior. And children of this age even went so far as to enforce these self-inferred norms when third parties “broke” them. These results suggest that children do not just passively acquire social norms from adult behavior and instruction; rather, they have a natural and proactive tendency to go from “is” to “ought.” That is, children go from observed actions to prescribed actions and do not perceive them simply as guidelines for their own behavior but rather as objective normative rules applying to everyone equally.</li>
</ul>
</li>
</ul>
<h2>05949</h2>
<ul>
<li>
<p><em>the racialized construction of exceptionality: experimental evidence of race/ethnicity effects on teachers' interventions</em><br />
rachel elizabeth fish 2016<br />
http://dx.doi.org/10.1016/j.ssresearch.2016.08.007<br />
Link: <a href="http://dx.doi.org/10.1016/j.ssresearch.2016.08.007">dx.doi.org/10.1016/j.ssresearch.2016.08.007</a></p>
<ul>
<li>Scholars, policy-makers, and practitioners have long argued that students of color are over-represented in special education and under-represented in gifted education, arguing that educators make racially/ethnically biased decisions to refer and qualify students with disabilities and giftedness. Recent research has called this into question, focusing on the role of confounders of race/ethnicity. However, the role of educator decisions in the disproportionality is still unclear. In this study, I examine the role of student race/ethnicity in teachers' categorization of student needs as “exceptional” and in need of special or gifted education services. I use an original survey experiment in which teachers read case studies of fictional male students in which the race/ethnicity, English Language Learner status, and exceptionality characteristics were experimentally manipulated. The teachers are then asked whether they would refer the student for exceptionality testing. My findings suggest a complex intersection of race/ethnicity and exceptionality, in which white boys are more likely to be suspected of having exceptionalities when they exhibit academic challenges, while boys of color are more likely to be suspected when they exhibit behavioral challenges. This suggests that the racialized construction of exceptionalities reflects differential academic expectations and interpretations of behavior by race/ethnicity, with implications for the subjectivity of exceptionality identification and for the exacerbation of racial/ethnic inequalities in education.</li>
</ul>
</li>
</ul>
<h2>06049</h2>
<ul>
<li><em>how to lie with statistics</em><br />
darrel huff 1954</li>
</ul>
<h2>06149</h2>
<ul>
<li>
<p><em>social class and the motivational relevance of other human beings: evidence from visual attention</em><br />
p. dietze, e. d. knowles 2016<br />
http://dx.doi.org/10.1177/0956797616667721<br />
Link: <a href="http://dx.doi.org/10.1177/0956797616667721">dx.doi.org/10.1177/0956797616667721</a></p>
<ul>
<li>We theorize that people’s social class affects their appraisals of others’ motivational relevance—the degree to which others are seen as potentially rewarding, threatening, or otherwise worth attending to. Supporting this account, three studies indicate that social classes differ in the amount of attention their members direct toward other human beings. In Study 1, wearable technology was used to film the visual fields of pedestrians on city streets; higher-class participants looked less at other people than did lower-class participants. In Studies 2a and 2b, participants’ eye movements were tracked while they viewed street scenes; higher class was associated with reduced attention to people in the images. In Study 3, a change-detection procedure assessed the degree to which human faces spontaneously attract visual attention; faces proved less effective at drawing the attention of high-class than low-class participants, which implies that class affects spontaneous relevance appraisals. The measurement and conceptualization of social class are discussed.</li>
</ul>
</li>
</ul>
<h2>06249</h2>
<ul>
<li>
<p><em>the threat of increasing diversity: why many white americans support trump in the 2016 presidential election</em><br />
b. major, a. blodorn, g. major blascovich 2016<br />
http://dx.doi.org/10.1177/1368430216677304<br />
Link: <a href="http://dx.doi.org/10.1177/1368430216677304">dx.doi.org/10.1177/1368430216677304</a></p>
<ul>
<li>What accounts for the widespread support for Donald Trump in the 2016 U.S. presidential race? This experiment demonstrates that the changing racial demographics of America contribute to Trump’s success as a presidential candidate among White Americans whose race/ethnicity is central to their identity. Reminding White Americans high in ethnic identification that non-White racial groups will outnumber Whites in the United States by 2042 caused them to become more concerned about the declining status and influence of White Americans as a group (i.e., experience group status threat), and caused them to report increased support for Trump and anti-immigrant policies, as well as greater opposition to political correctness. Increased group status threat mediated the effects of the racial shift condition on candidate support, anti-immigrant policy support, and opposition to political correctness. Among Whites low in ethnic identification, in contrast, the racial shift condition had no effect on group status threat or support for anti-immigrant policies, but did cause decreased positivity toward Trump and decreased opposition to political correctness. Group status threat did not mediate these effects. Reminders of the changing racial demographics had comparable effects for Democrats and Republicans. Results illustrate the importance of changing racial demographics and White ethnic identification in voter preferences and how social psychological theory can illuminate voter preferences.</li>
</ul>
</li>
</ul>
<h2>06349</h2>
<ul>
<li>
<p><em>behavioral and neural correlates to multisensory detection of sick humans</em><br />
christina regenbogen et al. 2017<br />
<a href="http://dx.doi.org/10.1073/pnas.1617357114">dx.doi.org/10.1073/pnas.1617357114</a></p>
<ul>
<li>
<p>In the perpetual race between evolving organisms and pathogens, the human immune system has evolved to reduce the harm of infections. As part of such a system, avoidance of contagious individuals would increase biological fitness. The present study shows that we can detect both facial and olfactory cues of sickness in others just hours after experimental activation of their immune system. The study further demonstrates that multisensory integration of these olfactory and visual sickness cues is a crucial mechanism for how we detect and socially evaluate sick individuals. Thus, by motivating the avoidance of sick conspecifics, olfactory–visual cues, both in isolation and integrated, may be important parts of circuits handling imminent threats of contagion.</p>
<p>Throughout human evolution, infectious diseases have been a primary cause of death. Detection of subtle cues indicating sickness and avoidance of sick conspecifics would therefore be an adaptive way of coping with an environment fraught with pathogens. This study determines how humans perceive and integrate early cues of sickness in conspecifics sampled just hours after the induction of immune system activation, and the underlying neural mechanisms for this detection. In a double-blind placebo-controlled crossover design, the immune system in 22 sample donors was transiently activated with an endotoxin injection [lipopolysaccharide (LPS)]. Facial photographs and body odor samples were taken from the same donors when “sick” (LPS-injected) and when “healthy” (saline-injected) and subsequently were presented to a separate group of participants (<em>n</em> = 30) who rated their liking of the presented person during fMRI scanning. Faces were less socially desirable when sick, and sick body odors tended to lower liking of the faces. Sickness status presented by odor and facial photograph resulted in increased neural activation of odor- and face-perception networks, respectively. A superadditive effect of olfactory–visual integration of sickness cues was found in the intraparietal sulcus, which was functionally connected to core areas of multisensory integration in the superior temporal sulcus and orbitofrontal cortex. Taken together, the results outline a disease-avoidance model in which neural mechanisms involved in the detection of disease cues and multisensory integration are vital parts.</p>
</li>
</ul>
</li>
</ul>
<h2>06449</h2>
<ul>
<li>
<p><em>seeing it both ways: openness to experience and binocular rivalry suppression</em><br />
anna antinori, olivia l. carter, luke d. smillie 2017<br />
<a href="http://dx.doi.org/10.1016/j.jrp.2017.03.005">dx.doi.org/10.1016/j.jrp.2017.03.005</a></p>
<ul>
<li>
<p>Demonstrates personality and mood can impact low-level perceptual experiences.<br />
Mixed percept, a binocular rivalry state, positively correlated with openness.<br />
Findings were replicated across samples and response bias was excluded.<br />
Used a perceptual-aesthetic mood induction that increased mixed in open people.</p>
<p>Openness to experience is characterised by flexible and inclusive cognition. Here we investigated whether this extends to basic visual perception, such that open people combine information more flexibly, even at low-levels of perceptual processing. We used binocular rivalry, where the brain alternates between perceptual solutions and times where neither solution is fully suppressed, mixed percept. Study 1 showed that openness is positively associated with duration of mixed percept and ruled out the possibility of response bias. Study 2 showed that mixed percept increased following a positive mood induction particularly for open people. Overall, the results showed that openness is linked to differences in low-level visual perceptual experience. Further studies should investigate whether this may be driven by common neural processes.</p>
</li>
</ul>
</li>
</ul>
<h2>06549</h2>
<ul>
<li>
<p>magic</p>
</li>
<li>
<p><em>now you see it, now you… seeing things that are hidden; failing to see things in plain sight. how magic exploits the everyday weirdness of perception</em><br />
vebjørn ekroll 2017<br />
<a href="https://aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things">aeon.co/essays/how-real-magic-happens-when-the-brain-sees-hidden-things</a></p>
</li>
<li>
<p><em>what brain-bending magic tricks can teach us about the mind</em><br />
rachel becker 2019<br />
<a href="http://theverge.com/2019/4/5/18297272/magic-psychology-optical-illusions-perception-cognition-experiencing-the-impossible">http://theverge.com/2019/4/5/18297272/magic-psychology-optical-illusions-perception-cognition-experiencing-the-impossible</a></p>
</li>
<li>
<p><em>experiencing the impossible: the science of magic</em><br />
gustav kuhn 2019 unread</p>
</li>
</ul>
<h2>06649</h2>
<ul>
<li>
<p><em>the order of disorder: deconstructing visual disorder and its effect on rule-breaking</em><br />
hiroki p. kotabe, omid kardan, marc g. berman 2016<br />
<a href="http://dx.doi.org/10.1037/xge0000240">xge0000240</a></p>
<ul>
<li>Disorderly environments are linked to disorderly behaviors. Broken windows theory (Wilson &amp; Kelling, 1982), an influential theory of crime and rule-breaking, assumes that scene-level social disorder cues (e.g., litter, graffiti) cause people to reason that they can get away with breaking rules. But what if part of the story is not about such complex social reasoning? Recent research suggests that basic visual disorder cues may be sufficient to encourage complex rule-breaking behavior. To test this hypothesis, we first conducted a set of experiments (Experiments 1–3) in which we identified basic visual disorder cues that generalize across visual stimuli with a variety of semantic content. Our results revealed that spatial features (e.g., nonstraight edges, asymmetry) are more important than color features (e.g., hue, saturation, value) for visual disorder. Exploiting this knowledge, we then reconstructed stimuli contrasted in terms of visual disorder, but absent of scene-level social disorder cues, to test whether visual disorder alone encourages cheating in a second set of experiments (Experiments 4 and 5). In these experiments, manipulating visual disorder increased the likelihood of cheating by up to 35% and the average magnitude of cheating by up to 87%. This work suggests that theories of rule-breaking that assume that complex social reasoning (e.g., about norms, policing, poverty) is necessary, should be reconsidered (e.g., Kelling &amp; Coles, 1997; Sampson &amp; Raudenbush, 2004). Furthermore, these experiments show that simple perceptual properties of the environment can affect complex behavior and sheds light on the extent to which our actions are within our control.</li>
</ul>
</li>
</ul>
<h2>06749</h2>
<ul>
<li>
<p><em>confirmation bias in human reinforcement learning: evidence from counterfactual feedback processing</em><br />
stefano palminteri et al. 2017<br />
<a href="https://doi.org/10.1371/journal.pcbi.1005684">doi.org/10.1371/journal.pcbi.1005684</a></p>
<ul>
<li>
<p>Previous studies suggest that <em>factual</em> learning, that is, learning from obtained outcomes, is biased, such that participants preferentially take into account positive, as compared to negative, prediction errors. However, whether or not the prediction error valence also affects <em>counterfactual</em> learning, that is, learning from forgone outcomes, is unknown. To address this question, we analysed the performance of two groups of participants on reinforcement learning tasks using a computational model that was adapted to test if prediction error valence influences learning. We carried out two experiments: in the factual learning experiment, participants learned from partial feedback (i.e., the outcome of the chosen option only); in the counterfactual learning experiment, participants learned from complete feedback information (i.e., the outcomes of both the chosen and unchosen option were displayed). In the factual learning experiment, we replicated previous findings of a valence-induced bias, whereby participants learned preferentially from positive, relative to negative, prediction errors. In contrast, for counterfactual learning, we found the opposite valence-induced bias: negative prediction errors were preferentially taken into account, relative to positive ones. When considering valence-induced bias in the context of both factual and counterfactual learning, it appears that people tend to preferentially take into account information that confirms their current choice.</p>
<p>While the investigation of decision-making biases has a long history in economics and psychology, learning biases have been much less systematically investigated. This is surprising as most of the choices we deal with in everyday life are recurrent, thus allowing learning to occur and therefore influencing future decision-making. Combining behavioural testing and computational modeling, here we show that the valence of an outcome biases both factual and counterfactual learning. When considering factual and counterfactual learning together, it appears that people tend to preferentially take into account information that confirms their current choice. Increasing our understanding of learning biases will enable the refinement of existing models of value-based decision-making.</p>
</li>
</ul>
</li>
<li>
<p><em>enduring extremes? polar vortex, drought, and climate change beliefs</em><br />
benjamin a. lyons et al. 2018<br />
<a href="http://dx.doi.org/10.1080/17524032.2018.1520735">http://dx.doi.org/10.1080/17524032.2018.1520735</a></p>
<ul>
<li><em>abstract</em> Some extreme weather events may be more likely to affect climate change beliefs than others, in part because schema individuals possess for different events could vary in encouraging such links. Using a representative sample of U.S. adults and geocoded National Weather Service data, we examine how a range of extreme weather event categories relate to climate change beliefs, and the degree to which individuals’ self-reported experiences are shaped by their political views across event types. For tornado, hurricane, and flood events, we find no link with beliefs. For polar vortex and drought events, we find that although self-reported experience is linked with climate beliefs, reporting of these experiences is influenced by political identity and partisan news exposure. These findings underscore a limited role for extreme weather experiences in climate beliefs, and show that events more open to interpretation, such as droughts and polar vortex disturbances, are most likely to be seen through a partisan lens.</li>
<li>“Extreme weather plays a limited long-term role in forming people’s beliefs about climate change. Instead, their views and beliefs can alter the way they perceive the weather. We have found when an extreme weather event is ambiguous, as with polar vortex and drought, people are more likely to see the event through a partisan lens. If there is grey area, people are more comfortable applying their preferred label.”<br />
The University of Exeter, University of Michigan and University of Texas research found that Republicans were less likely to report experiencing a polar vortex, while those exposed to liberal media were more likely.<br />
However the weather can be sometimes so extreme that it overshadows personal views — the researchers found that partisanship and media use did not affect the way people in the American Northeast — where the 2014 and 2015 polar vortex events hit hardest — reported the weather they had experienced.<br />
Those who favoured liberal news sources such as the Huffington Post or the Daily Show reported experiencing drought more often than national weather data would suggest they actually did.<br />
Dr Lyons said: “Very extreme weather accompanied by constant media coverage is harder for people to deny. But on the other end of the scale, droughts can take longer to have an effect, so people have some difficulty perceiving their onset and this may allow them to bring their biases to the table.”<br />
Academics surveyed 3,057 people in the USA to ask them about the extreme weather they had experienced over a five-year period, and also if they believed in climate change, human causation, and the scientific consensus on the matter. They also asked where they lived. The experts were then able to compare these answers to official weather reports for that region for the same time period.<br />
Data about the weather was taken from the Storm Events Database compiled by NOAA’s National Weather Service (NWS). The data included droughts, floods, tornadoes, and hurricanes. A total of 21.7 per cent of respondents reported experiencing a polar vortex, 41.0 per cent a drought, 19.8 per cent a tornado, 29.3 per cent flood, and 16.7 per cent a hurricane in the past five years. However the data shows 21.3 per cent lived in a county where a flood was recorded over the time period, 25.3 per cent a tornado, 4.3 per cent a hurricane, and 4.4 per cent drought.<br />
A total of 59.2 per cent of respondents agreed that “there is solid evidence that the average temperature on earth has been getting warmer over the past few decades.” Of respondents who agreed with this statement, 74.2 per cent agreed that the Earth was warming mostly due to “human activity such as burning fossil fuels.”<br />
Dr Lyons said: “This research shows people’s perception of extreme weather can be processed through partisan lenses. This means efforts to connect extreme events with climate change may do more to rally those with liberal beliefs than convince those with more conservative views that humans are having an impact on the climate. “However, it’s important to note that we take a big-picture look rather than focus on specific events. Particularly intense events — a 100-year flood or catastrophic hurricane — might be most capable of influencing attitudes.”</li>
</ul>
</li>
</ul>
<h2>06849</h2>
<ul>
<li>
<p><em>a drama movie activates brains of holistic and analytical thinkers differentially</em><br />
mareike bacha-trams et al. 2018<br />
<a href="http://dx.doi.org/10.1093/scan/nsy099">http://dx.doi.org/10.1093/scan/nsy099</a></p>
<ul>
<li>
<p>Aalto University researchers showed volunteers the film My Sister's Keeper on a screen while the research subjects were lying down in an MRI scanner. The study compared the volunteers' brain activity, and concluded that holistic thinkers saw the film more similarly with each other than analytical thinkers. In addition, holistic thinkers processed the film's moral issues and factual connections within the film more similarly with each other than the analytical thinkers.</p>
<p>Before conducting the MRI scan, the 26 persons participating in the research were divided into holistic and analytical thinkers on the basis of a previously established evaluation survey. According to previous studies, analytical thinkers pay attention to objects and persons while looking at photographs, whereas holistic thinkers consider also the background and context.</p>
<p>'Holistic thinkers showed more similarities in extensive areas of the cerebral cortex than analytical thinkers. This suggests that holistic thinkers perceive a film more similarly with each other than analytical thinkers,' says Professor Iiro Jääskeläinen.</p>
<p>Significantly more similarity was observed in holistic thinkers in the parts of the brain generally related to moral processing -- in the occipital, prefrontal and anterior parts of the temporal cortices. This suggests the holistic thinkers processed the moral questions of My Sister's Keeper in a similar way to one another. The anterior parts of the temporal lobes, however, process meanings of words.</p>
<p>Analytical thinkers showed similarities mainly in the sensory and auditory parts of the brain. They listen to the dialogue literally, whilst holistic thinkers perceive the meanings through the context and their own interpretation of the film's narrative.</p>
<p>'It was surprising to find so many large differences in so many cerebral areas between the groups.' Professor Jääskeläinen said. 'Analytical and holistic thinkers clearly see the world and events in very different ways. On the basis of the visual cortex, it can still be concluded that holistic thinkers follow the film scenes more similarly, whereas analytical thinkers are more individual and focus more on details.'</p>
<p>So far, research dealing with analytical and holistic views has focused on cultural differences between the east and west: more analytical thinking has been detected in western cultures, and more holistic thinking in eastern cultures. Now the study was carried out within one culture and, for the first time, as a film study.</p>
<p>'The research can help people understand the way other people observe the world. A holistic thinker may find it frustrating that an analytical thinker interprets things literally, sticks to details and does not see the big picture or context. An analytical thinker may, on the other hand, see the holistic thinker as a superstitious person, who believes in long causal links, such as the butterfly effect.'</p>
</li>
<li>
<p><em>abstract</em> People socialized in different cultures differ in their thinking styles. Eastern-culture people view objects more holistically by taking context into account, whereas Western-culture people view objects more analytically by focusing on them at the expense of context. Here we studied whether participants, who have different thinking styles but live within the same culture, exhibit differential brain activity when viewing a drama movie. A total of 26 Finnish participants, who were divided into holistic and analytical thinkers based on self-report questionnaire scores, watched a shortened drama movie during functional magnetic resonance imaging. We compared intersubject correlation (ISC) of brain hemodynamic activity of holistic vs analytical participants across the movie viewings. Holistic thinkers showed significant ISC in more extensive cortical areas than analytical thinkers, suggesting that they perceived the movie in a more similar fashion. Significantly higher ISC was observed in holistic thinkers in occipital, prefrontal and temporal cortices. In analytical thinkers, significant ISC was observed in right-hemisphere fusiform gyrus, temporoparietal junction and frontal cortex. Since these results were obtained in participants with similar cultural background, they are less prone to confounds by other possible cultural differences. Overall, our results show how brain activity in holistic vs analytical participants differs when viewing the same drama movie.</p>
</li>
</ul>
</li>
<li>
<p><em>differential inter-subject correlation of brain activity when kinship is a variable in moral dilemma</em><br />
mareike bacha-trams et al. 2018<br />
<a href="http://dx.doi.org/10.1038/s41598-017-14323-x">http://dx.doi.org/10.1038/s41598-017-14323-x</a></p>
<ul>
<li><em>abstract</em> Previous behavioural studies have shown that humans act more altruistically towards kin. Whether and how knowledge of genetic relatedness translates into differential neurocognitive evaluation of observed social interactions has remained an open question. Here, we investigated how the human brain is engaged when viewing a moral dilemma between genetic vs. non-genetic sisters. During functional magnetic resonance imaging, a movie was shown, depicting refusal of organ donation between two sisters, with subjects guided to believe the sisters were related either genetically or by adoption. Although 90% of the subjects self-reported that genetic relationship was not relevant, their brain activity told a different story. Comparing correlations of brain activity across all subject pairs between the two viewing conditions, we found significantly stronger inter-subject correlations in insula, cingulate, medial and lateral prefrontal, superior temporal, and superior parietal cortices, when the subjects believed that the sisters were genetically related. Cognitive functions previously associated with these areas include moral and emotional conflict regulation, decision making, and mentalizing, suggesting more similar engagement of such functions when observing refusal of altruism from a genetic sister. Our results show that mere knowledge of a genetic relationship between interacting persons robustly modulates social cognition of the perceiver.</li>
</ul>
</li>
</ul>
<h2>06949</h2>
<ul>
<li>
<p><em>is overconfidence a social liability? the effect of verbal versus nonverbal expressions of confidence</em><br />
elizabeth r. tenney et al. 2018<br />
<a href="http://dx.doi.org/10.1037/pspi0000150">http://dx.doi.org/10.1037/pspi0000150</a></p>
<ul>
<li>
<p>The team conducted a series of experiments in which participants met potential collaborators or advisers and decided which — the confident or cautious — they trusted and wanted to work with most. On average, they strongly preferred the confident candidate; however, once they learned that person was overconfident and the cautious counterpart was well-calibrated, caution won.<br />
“Interestingly, though, we found that if the overly confident candidates expressed their confidence nonverbally, they remained the most trusted and desirable choice, even when revealed to be over-the-top,” Meikle says.<br />
The findings illustrate how politicians, business leaders and others are able to retain their status and influence even when they are potentially exposed as being overconfident: by leveraging plausible deniability — their ability to deny responsibility due to a lack of concrete evidence.<br />
“The plausible deniability hypothesis explains why overconfidence sometimes, but not always, is punished,” Meikle says. “For example, verifiably overconfident claims, void of plausible deniability, will face consequences. However, there are a number of ways people can create plausible deniability.”</p>
</li>
<li>
<p><em>abstract</em> What are the reputational consequences of being overconfident? We propose that the channel of confidence expression is one key moderator—that is, whether confidence is expressed verbally or nonverbally. In a series of experiments, participants assessed target individuals (potential collaborators or advisors) who were either overconfident or cautious. Targets expressed confidence, or a lack thereof, verbally or nonverbally. Participants then learned targets’ actual performance. Across studies, overconfidence was advantageous initially—regardless of whether targets expressed confidence verbally or nonverbally. After performance was revealed, overconfident targets who had expressed confidence verbally were viewed more negatively than cautious targets; however, overconfident targets who had expressed confidence nonverbally were still viewed more positively than cautious ones. The one condition wherein nonverbal overconfidence was detrimental was when confidence was clearly tied to a falsifiable claim. Results suggest that, compared with verbal statements, nonverbal overconfidence reaps reputational benefits because of its plausible deniability.</p>
</li>
</ul>
</li>
</ul>
<h2>07049</h2>
<ul>
<li>
<p><em>pre–suasion: a revolutionary way to influence and persuade</em><br />
robert cialdini 2016 9781501109812</p>
<ul>
<li>the effectiveness of persuasive messages will be drastically affected by the type of opener experienced immediately in advance</li>
<li>Put people in a wary state of mind via that opener, and, driven by a desire for safety, a popularity-based appeal will soar, whereas a distinctiveness-based appeal will sink. But use it to put people in an amorous state of mind, and, driven by a consequent desire to stand out, the reverse will occur.</li>
<li>salience–importance confounding</li>
<li>focal–causal confounding</li>
<li>orientation reflex
<ul>
<li>“a persuasion-oriented producer, writer, or director needs to be concerned principally with shots and cuts”</li>
<li>“cuts are crucial to persuasive success because they can be manipulated to bring into focus the feature of a message the persuader believes to be most convincing—by shifting the scene to that feature. That cut will instigate an orienting response to the winning feature in audience members’ brains before they even experience it.”</li>
<li>create background of similar things draws attention to that which is different and hence perceived importance</li>
</ul>
</li>
<li>unresolved
<ul>
<li>mystery</li>
</ul>
</li>
<li>“the main purpose of speech is to direct listeners’ attention to a selected sector of reality. Once that is accomplished, the listeners’ existing associations to the now-spotlighted sector will take over to determine the reaction.”</li>
<li><em>influence: science and practice</em><br />
robert cialdini</li>
</ul>
</li>
<li>
<p><em>surprise, recipes for surprise, and social influence</em><br />
jeffrey loewenstein 2018<br />
<a href="http://dx.doi.org/10.1111/tops.12312">http://dx.doi.org/10.1111/tops.12312</a></p>
<ul>
<li>Surprising people can provide an opening for influencing them. Surprises garner attention, are arousing, are memorable, and can prompt shifts in understanding. Less noted is that, as a result, sur- prises can serve to persuade others by leading them to shifts in attitudes. Furthermore, because stories, pictures, and music can generate surprises and those can be widely shared, surprise can have broad social influence. People also tend to share surprising items with others, as anyone on social media has discovered. This means that in addition to broadcasting surprising information, surprising items can also spread through networks. The joint result is that surprise not only has individual effects on beliefs and attitudes but also collective effects on the content of culture. Items that generate sur- prise need not be random or accidental. There are predictable methods or recipes for generating surprise. One such recipe is discussed, the <em>repetition-break plot structure</em>, to explore the psychological and social possibilities of examining surprise. Recipes for surprise offer a useful means for under- standing how surprise works and offer prospects for harnessing surprise to a wide array of ends.</li>
</ul>
</li>
</ul>
<h2>07149</h2>
<ul>
<li>
<p><em>perceived entitlement causes discrimination against attractive job candidates in the domain of relatively less desirable jobs</em><br />
margaret lee et al. 2017<br />
<a href="http://dx.doi.org/10.1037/pspi0000114">dx.doi.org/10.1037/pspi0000114</a></p>
<ul>
<li>People generally hold positive stereotypes of physically attractive people and because of those stereotypes often treat them more favorably. However, we propose that some beliefs about attractive people, specifically, the perception that attractive individuals have a greater sense of entitlement than less attractive individuals, can result in negative treatment of attractive people. We examine this in the context of job selection and propose that for relatively less desirable jobs, attractive candidates will be discriminated against. We argue that the ascribed sense of entitlement to good outcomes leads to perceptions that attractive individuals are more likely to be dissatisfied working in relatively less desirable jobs. When selecting candidates for relatively less desirable jobs, decision makers try to ascertain whether a candidate would be satisfied in those jobs, and the stereotype of attractive individuals feeling entitled to good outcomes makes decision makers judge attractive candidates as more likely to be dissatisfied in relatively less (but not more) desirable jobs. Consequently, attractive candidates are discriminated against in the selection for relatively less desirable jobs. Four experiments found support for this theory. Our results suggest that different discriminatory processes operate when decision makers select among candidates for relatively less desirable jobs and that attractive people might be systematically discriminated against in a segment of the workforce.</li>
</ul>
</li>
</ul>
<h2>07249</h2>
<ul>
<li><em>’fiction is outperforming reality’: how youtube’s algorithm distorts truth</em><br />
paul lewis 2018<br />
<a href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth">https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth</a></li>
</ul>
<h2>07349</h2>
<ul>
<li>
<p><em>déjà vu: an illusion of prediction</em><br />
anne m. cleary, alexander b. claxton 2018<br />
<a href="http://dx.doi.org/10.1177/0956797617743018">http://dx.doi.org/10.1177/0956797617743018</a></p>
<ul>
<li>Déjà vu is beginning to be scientifically understood as a memory phenomenon. Despite recent scientific advances, a remaining puzzle is the purported association between déjà vu and feelings of premonition. Building on research showing that déjà vu can be driven by an unrecalled memory of a past experience that relates to the current situation, we sought evidence of memory-based predictive ability during déjà vu states. Déjà vu did not lead to above-chance ability to predict the next turn in a navigational path resembling a previously experienced but unrecalled path (although such resemblance increased reports of déjà vu). However, déjà vu states were accompanied by increased feelings of knowing the direction of the next turn. The results suggest that feelings of premonition during déjà vu occur and can be illusory. Metacognitive bias brought on by the state itself may explain the peculiar association between déjà vu and the feeling of premonition.</li>
</ul>
</li>
<li>
<p><em>what you saw is what you will hear: two new illusions with audiovisual postdictive effects</em><br />
noelle r. b. stiles et al. 2018<br />
<a href="http://dx.doi.org/10.1371/journal.pone.0204217">http://dx.doi.org/10.1371/journal.pone.0204217</a></p>
<ul>
<li>Neuroscience investigations are most often focused on the prediction of future perception or decisions based on <em>prior</em> brain states or stimulus presentations. However, the brain can also process information retroactively, such that <em>later</em> stimuli impact conscious percepts of the stimuli that have already occurred (called “postdiction”). Postdictive effects have thus far been mostly unimodal (such as apparent motion), and the models for postdiction have accordingly been limited to early sensory regions of one modality. We have discovered two related multimodal illusions in which audition instigates postdictive changes in visual perception. In the first illusion (called the “Illusory Audiovisual Rabbit”), the location of an illusory flash is influenced by an auditory beep-flash pair that follows the perceived illusory flash. In the second illusion (called the “Invisible Audiovisual Rabbit”), a beep-flash pair following a real flash suppresses the perception of the earlier flash. Thus, we showed experimentally that these two effects are influenced significantly by postdiction. The audiovisual rabbit illusions indicate that postdiction can bridge the senses, uncovering a relatively-neglected yet critical type of neural processing underlying perceptual awareness. Furthermore, these two new illusions broaden the Double Flash Illusion, in which a single real flash is doubled by two sounds. Whereas the double flash indicated that audition can create an illusory flash, these rabbit illusions expand audition’s influence on vision to the suppression of a real flash and the relocation of an illusory flash. These new additions to auditory-visual interactions indicate a spatio-temporally fine-tuned coupling of the senses to generate perception.</li>
</ul>
</li>
</ul>
<h2>07449</h2>
<ul>
<li>
<p><em>rude color glasses: the contaminating effects of witnessed morning rudeness on perceptions and behaviors throughout the workday</em><br />
woolum, andrew et al. 2017<br />
<a href="http://psycnet.apa.org/doi/10.1037/apl0000247">http://psycnet.apa.org/doi/10.1037/apl0000247</a></p>
<ul>
<li>Using an experimental experience sampling design, we investigate how witnessing morning rudeness influences workers’ subsequent perceptions and behaviors throughout the workday. We posit that a single exposure to rudeness in the morning can contaminate employees’ perceptions of subsequent social interactions leading them to perceive greater workplace rudeness throughout their workday. We expect that these contaminated perceptions will have important ramifications for employees’ work behaviors. In a 10-day study of 81 professional and managerial employees, we find that witnessed morning rudeness leads to greater perceptions of workplace rudeness throughout the workday and that those perceptions, in turn, predict lower task performance and goal progress and greater interaction avoidance and psychological withdrawal. We also find that the contaminating effect of morning rudeness depends on core self-evaluations (CSE)—employees high (vs. low) in CSE are affected less by exposure to morning rudeness. We discuss implications for practice and theory.</li>
</ul>
</li>
<li>
<p><em>coffee cues elevate arousal and reduce level of construal</em><br />
eugene y. chan, sam j. maglio 2019<br />
<a href="http://dx.doi.org/10.1016/j.concog.2019.02.007">http://dx.doi.org/10.1016/j.concog.2019.02.007</a></p>
<ul>
<li>
<p>&quot;As long as individuals see a connection between coffee and arousal, whatever its origin may be, mere exposure to coffee-related cues might trigger arousal in and of themselves without ingesting any form of caffeine,&quot; Dr Chan said.</p>
<p>&quot;Smelling coffee gives rise to the beverage's psychoactive, arousing effects. This is because the brains of habitual coffee consumers are conditioned to respond to coffee in certain ways, as per the prominent Pavlov's dog theory.</p>
<p>&quot;So walking past your favourite café, smelling the odours of coffee grounds, or even witnessing coffee-related cues in the form of advertising can trigger the chemical receptors in our body enough for us to obtain the same arousal sensations without consumption.&quot;</p>
<p>Researchers exposed 871 participants from Western and Eastern cultures to coffee and tea-related cues across four separate experiments that would make them think of the substance without actually ingesting it.</p>
<p>In one study, participants had to come up with advertising slogans for coffee or tea. In another, they had to mock-up news stories about the health benefits of drinking coffee or tea. The arousal levels and heart rates were monitored by the researchers throughout the studies.</p>
<p>The study centred on a psychological effect called 'mental construal'. This determines how individuals think and process information, whether they focus on narrow details or the bigger picture.</p>
<p>Results showed that priming people with coffee cues -- exposing them to images and other stimuli (smells and sounds) about coffee -- increased their alertness, energy levels, heart rate, and made them think narrowly.</p>
<p>The cognitive-altering effects of coffee were more prevalent in participants from Western countries, where coffee is more popular and has connotations related to energy, focus and ambition, compared to those from Eastern countries. Coffee was also associated with greater arousal than tea.</p>
<p>&quot;Our research can offer intriguing implications, as it relies not on physiology but rather psychological associations to change our cognitive patterns,&quot; Dr Chan said.</p>
<p>&quot;This study could even help to explain how drinking decaffeinated coffee can produce faster reaction times on tasks. Perhaps the mental association between coffee and arousal is so strong that it can produce cognitive changes even where there's no caffeine ingestion physiologically.</p>
<p>&quot;This adds to the growing amount of literature documenting that the foods we eat and the beverages we drink do more than simply provide nutrition or pleasure -- mere exposure to, or reminders of them, affect how we think.&quot;</p>
</li>
<li>
<p><em>abstract</em> •In Western societies, coffee is associated with greater arousal than tea.<br />
•Thus, exposure to coffee- (vs. tea-) related cues should increase arousal and lower mental construal level.<br />
•We conducted four experiments to test this hypothesis, presenting participants with cues related to either coffee or tea.<br />
•The results suggest that exposure to coffee cues can lead to a concrete construal via greater arousal.<br />
•The effects arise even without actually drinking coffee or tea.</p>
<p>Coffee and tea are two beverages commonly-consumed around the world. Therefore, there is much research regarding their physiological effects. However, less is known about their psychological meanings. Derived from a predicted lay association between coffee and arousal, we posit that exposure to coffee-related cues should increase arousal, even in the absence of actual ingestion, relative to exposure to tea-related cues. We further suggest that higher arousal levels should facilitate a concrete level of mental construal as conceptualized by Construal Level Theory. In four experiments, we find that coffee cues prompted participants to see temporal distances as shorter and to think in more concrete, precise terms. Both subjective and physiological arousal explain the effects. We situate our work in the literature that connects food and beverage to cognition or decision-making. We also discuss the applied relevance of our results as coffee and tea are among the most prevalent beverages globally.</p>
</li>
</ul>
</li>
</ul>
<h2>07549</h2>
<ul>
<li>
<p><em>expertise fails to attenuate gendered biases in judicial decision-making</em><br />
andrea l. miller 2018<br />
<a href="http://dx.doi.org/10.1177/1948550617741181">http://dx.doi.org/10.1177/1948550617741181</a></p>
<ul>
<li>Although the influence of gender ideology on lay decision-making has been established, it is not known to what extent expertise may mitigate gendered biases and improve decision-making quality. In a set of controlled experiments, trial court judges and laypeople evaluated a hypothetical child custody case and a hypothetical employment discrimination case. The role of expertise was tested in two ways: by comparing judges’ and laypeople’s decision-making and by examining relative differences in expertise among judges. Judges were no less influenced by litigant gender and by their own gender ideology than the lay sample. Judges with greater subject-matter expertise were also no less influenced by gender ideology than other judges. In some cases, expertise was associated with greater, not less, bias. The results of this study suggest that expertise does not attenuate gendered biases in legal decision-making.</li>
</ul>
</li>
</ul>
<h2>07649</h2>
<ul>
<li>
<p><em>emotion sensitivity across the lifespan: mapping clinical risk periods to sensitivity to facial emotion intensity</em><br />
lauren a. rutter et al. 2019<br />
<a href="http://dx.doi.org/10.1037/xge0000559">http://dx.doi.org/10.1037/xge0000559</a></p>
<ul>
<li>
<p>For the study, researchers created a digital test of emotion sensitivity that was completed by nearly 10,000 men and women, ranging in age from 10 to 85. The test allowed researchers to measure how much each person was able to detect subtle differences in facial cues of fear, anger, and happiness. The test also identified how people in different age groups displayed changes in their sensitivity to those facial emotions.</p>
<p>Rutter, the study's lead author and a research fellow at McLean Hospital's Laboratory for Brain and Cognitive Health Technology, explained that participants were tested using the web-based platform TestMyBrain.org. They were shown images of faces, presented in pairs, and were asked &quot;Which face is more angry?,&quot; &quot;Which face is more happy?,&quot; or &quot;Which face is more fearful?&quot; Rutter stated that the online platform helped the researchers tap into a &quot;much larger and more diverse sample set&quot; than previous studies. She also said that the novel testing method helped improve the accuracy of the results for decoding facial cues.</p>
<p>Germine, the study's senior author, said that the new testing method and the large sample size helped the researchers gain a deeper understanding into differences in emotion processing. &quot;From studies and anecdotal evidence, we know that the everyday experiences of an adolescent is different from a middle aged or older person, but we wanted to understand how these experiences might be linked with differences in basic emotion understanding,&quot; said Germine, who is the technical director of the McLean Institute for Technology in Psychiatry and director of the Laboratory for Brain and Cognitive Health Technology. Rutter added that &quot;the paper grew out of knowing that these differences existed and wanting to compare these differences across the emotion categories.&quot;</p>
<p>Through their study, the researchers also drilled down on the way emotion sensitivity develops during adolescence.</p>
<p>&quot;We found that sensitivity to anger cues improves dramatically during early to mid-adolescence,&quot; said Rutter. &quot;This is the exact age when young people are most attuned to forms of social threat, such as bullying. The normal development of anger sensitivity can contribute to some of the challenges that arise during this phase of development.&quot;</p>
<p>On the other end of the life span, the study showed that sensitivity to facial cues for fear and anger decrease as people age, but the ability to detect happiness cues stays the same. &quot;It's well established that there is an age-related decline in the ability to decode emotion cues, in general, but here we see very little decline in the ability to detect differences in happiness,&quot; Germine said. This is even though the study was designed to be sensitive to differences in happiness sensitivity with age, based on principles from psychometrics and signal detection theory.</p>
<p>&quot;What's remarkable is that we see declines in many visual perceptual abilities as we get older, but here we did not see such declines in the perception of happiness,&quot; she said. &quot;These findings fit well with other research showing that older adults tend to have more positive emotions and a positive outlook.&quot;</p>
<p>Now, the researchers are building on this study by conducting new work that examines how emotion sensitivity is related to differences in aspects of mental health, such as anxiety. The team is also looking at how sensitivity to anger and happiness cues might be related to the development of poorer mental health after trauma.</p>
</li>
<li>
<p><em>abstract</em> Face emotion perception is important for social functioning and mental health. In addition to recognizing categories of face emotion, accurate emotion perception relies on the ability to detect subtle differences in emotion intensity. The primary aim of this study was to examine participants’ ability to discriminate the intensity of facial emotions (emotion sensitivity: ES) in three psychometrically matched ES tasks (fear, anger, or happiness), to identify developmental changes in sensitivity to face emotion intensity across the lifespan. We predicted that increased age would be associated with lower anger and fear ES, with minimal differences in happiness ES. Participants were 9,546 responders to a Web-based ES study (age range = 10 to 85 years old). Results of segmented linear regression confirmed our hypotheses and revealed differential patterns of ES based on age, sex, and emotion category. Females showed enhanced sensitivity to anger and fear relative to males, but similar sensitivity to happiness. While sensitivity to all emotions increased during adolescence and early adulthood, sensitivity to anger showed the largest increase, potentially related to the importance of anger perception during adolescent development. We also observed age-related decreases in both anger and fear sensitivity in older adults, with little to no change in happiness sensitivity. Unlike previous studies, the effect observed here could not be explained by task-related confounds (e.g., ceiling effects for happiness recognition), lending strong support to observed differences in ES for happiness, anger, and fear across age. Implications for everyday functioning and the development of psychopathology across the lifespan are discussed.</p>
</li>
</ul>
</li>
</ul>
<h2>07749</h2>
<ul>
<li>
<p><em>prevalence-induced concept change in human judgment</em><br />
david e. levari et al. 2018<br />
<a href="http://dx.doi.org/10.1126/science.aap8731">http://dx.doi.org/10.1126/science.aap8731</a></p>
<ul>
<li>Why do some social problems seem so intractable? In a series of experiments, we show that people often respond to decreases in the prevalence of a stimulus by expanding their concept of it. When blue dots became rare, participants began to see purple dots as blue; when threatening faces became rare, participants began to see neutral faces as threatening; and when unethical requests became rare, participants began to see innocuous requests as unethical. This “prevalence-induced concept change” occurred even when participants were forewarned about it and even when they were instructed and paid to resist it. Social problems may seem intractable in part because reductions in their prevalence lead people to see more of them.</li>
</ul>
</li>
</ul>
<h2>07849</h2>
<ul>
<li>
<p><em>anomaly detection in paleoclimate records using permutation entropy</em><br />
joshua garland et al. 2018<br />
<a href="http://dx.doi.org/10.3390/e20120931">http://dx.doi.org/10.3390/e20120931</a></p>
<ul>
<li>
<p>When making sense of the massive amount of information packed into an ice core, scientists face a forensic challenge: how best to separate the useful information from the corrupt.</p>
<p>A new paper published in the journal Entropy shows how tools from information theory, a branch of complexity science, can address this challenge by quickly homing in on portions of the data that require further investigation.</p>
<p>&quot;With this kind of data, we have limited opportunities to get it right,&quot; says Joshua Garland, a mathematician at the Santa Fe Institute who works with 68,000 years of data from the West Antarctic Ice Sheet Divide ice Core. &quot;Extracting the ice and processing the data takes hundreds of people, and tons of processing and analysis. Because of resource constraints, replicate cores are rare. &quot;</p>
<p>By the time Garland and his team got ahold of the data, more than 10 years had passed from the initial drilling of the ice core to the publishing of the dataset it contained. The two-mile ice core was extracted over five seasons from 2007-2012, by teams from the multiple universities funded by the National Science Foundation. From the field camp in West Antarctica, the core was packaged, then shipped to the National Science Foundation Ice Core Facility in Colorado, and finally to the University of Colorado. At the Stable Isotope Lab at the Institute of Arctic and Alpine Research, a state-of-the-art processing facility helped scientists pull water isotope records from the ice.</p>
<p>The result is a highly resolved, complex dataset. Compared to previous ice core data, which allowed for analysis every 5 centimeters, the WAIS Divide core permits analysis at millimeter resolution.</p>
<p>&quot;One of the exciting thing about ice core research in the last decade is we've developed these lab systems to analyze the ice in high resolution,&quot; says Tyler Jones, a paleoclimatologist at the University of Colorado Boulder. &quot;Quite a while back we were limited in our ability to analyze climate because we couldn't get enough data points, or if we could it would take too long. These new techniques have given us millions of data points, which is rather difficult to manage and interpret without some new advances in our [data] processing.&quot;</p>
<p>In previous cores, Garland notes that decades, even centuries, were aggregated into a single point. The WAIS data, by contrast, sometimes gives more than forty data points per year. But as scientists move to analyze the data at shorter time scales, even small anomalies can be problematic.</p>
<p>&quot;As fine-grained data becomes available, fine-grained analyses can be performed,&quot; Garland notes. &quot;But it also makes the analysis susceptible to fine-grained anomalies.&quot;</p>
<p>To quickly identify which anomalies require further investigation, the team uses information theoretic techniques to measure how much complexity appears at each point in the time sequence. A sudden spike in the complexity could mean that there was either a major, unexpected climate event, like a super volcano, or that there was an issue in the data or the data processing pipeline.</p>
<p>&quot;This kind of anomaly would be invisible without a highly detailed, fine-grained, point-by-point analysis of the data, which would take a human expert many months to perform,&quot; says Elizabeth Bradley, a computer scientist at the University of Colorado Boulder and External Professor at the Santa Fe Institute. &quot;Even though information theory can't tell us the underlying cause of an anomaly, we can use these techniques to quickly flag the segments of the data set that should be investigated by paleoclimate experts.&quot;</p>
<p>She compares the ice core dataset to a Google search that returns a million pages. &quot;It's not that you couldn't go through those million pages,&quot; Bradley says. &quot;But imagine if you had a technique that could point you toward the ones that were potentially meaningful?&quot; When analyzing large, real-world datasets, information theory can spot differences in the data that signal either a processing error or a significant climate event.</p>
<p>In their Entropy paper, the scientists detail how they used information theory to identify and repair a problematic stretch of data from the original ice core. Their investigation eventually prompted a resampling of the archival ice core -- the longest resampling of a high-resolution ice core to date. When that portion of the ice was resampled and reprocessed, the team was able to resolve an anomalous spike in entropy from roughly 5,000 years ago.</p>
<p>&quot;It's vitally important to get this area right,&quot; Garland notes, &quot;because it contains climate information from the dawn of human civilization.&quot;</p>
<p>&quot;I think climate change is the most pressing problem ever to face humanity, and ice cores are undoubtedly the best record of Earth's climate going back hundreds of thousands of years,&quot; says Jones. &quot;Information theory helps us sift through the data to make sure what we're putting out into the world is the absolute best and most certain product we can.&quot;</p>
</li>
<li>
<p><em>abstract</em> Permutation entropy techniques can be useful for identifying anomalies in paleoclimate data records, including noise, outliers, and post-processing issues. We demonstrate this using weighted and unweighted permutation entropy with water-isotope records containing data from a deep polar ice core. In one region of these isotope records, our previous calculations (See Garland et al. 2018) revealed an abrupt change in the complexity of the traces: specifically, in the amount of new information that appeared at every time step. We conjectured that this effect was due to noise introduced by an older laboratory instrument. In this paper, we validate that conjecture by reanalyzing a section of the ice core using a more advanced version of the laboratory instrument. The anomalous noise levels are absent from the permutation entropy traces of the new data. In other sections of the core, we show that permutation entropy techniques can be used to identify anomalies in the data that are not associated with climatic or glaciological processes, but rather effects occurring during field work, laboratory analysis, or data post-processing. These examples make it clear that permutation entropy is a useful forensic tool for identifying sections of data that require targeted reanalysis—and can even be useful for guiding that analysis.</p>
</li>
</ul>
</li>
</ul>
<h2>07949</h2>
<ul>
<li>
<p>intention</p>
</li>
<li>
<p><em>cues to intention bias action perception toward the most efficient trajectory</em><br />
katrina l. mcdonough et al. 2019<br />
<a href="http://dx.doi.org/10.1038/s41598-019-42204-y">http://dx.doi.org/10.1038/s41598-019-42204-y</a></p>
<ul>
<li>
<p>the way humans &quot;see&quot; the actions of others is slightly distorted by their expectations.</p>
<p>The new study shows these changes really reflect the intentions we attribute to others, and specifically happen when watching people but not other objects.</p>
<p>For the study, participants watched brief videos of an actor starting to reach either straight for an object or making an arched reach over an obstacle. Before the action was complete, the hand suddenly disappeared, and participants identified the point on the touch screen where they last saw it.</p>
<p>The trick was that in some trials, the hand made an arched reach even though there was no obstacle or started to reach straight even though an obstacle was in the way, so that it would knock into it. In other words, what people saw was clearly in conflict with their expectations about how people typically act.</p>
<p>As in the authors' earlier work, the results showed people were able to judge the expected actions accurately. The perception of unexpected ones was, however, subtly coloured by their expectations.</p>
<p>For unexpected straight reaches, people reported the hand disappeared slightly higher than it really did, as if they &quot;saw&quot; it start to avoid the obstacle even though it clearly did not.</p>
<p>Similarly, if there was no obstacle to avoid, high arched reaches were reported slightly lower, as if people saw a straighter action than was really shown. In other words, people tended to see the actions as they expected, but not as they really were.</p>
<p>The question was what would happen in another group of participants that did not watch moving hands, but balls -- objects to which no goals are typically attributed, and certainly no tendencies to avoid obstacles. When these participants reported what they had seen, they did not make these mis-judgments, particularly when the ball did not move in a biological, human-like way.</p>
<p>The new results therefore show that it is really the intentions we attribute to other people -- but not objects -- that lead us to mis-perceive their actions.</p>
<p>Lead author Katrina L McDonough, a PhD candidate at the University, said: &quot;The misjudgements we found were not large. People did not see a completely different movement than was really there, but even the subtle changes we measured could have large impacts in everyday life. If we see a person behaving ambiguously, for example, such small changes may be enough to make us interpret the behaviour differently or cause us to miss the true intention behind it.&quot;</p>
<p>Dr Patric Bach, Associate Professor and Head of the Action Prediction Lab, added: &quot;While this study was conducted with typically developing participants, it may provide new avenues for understanding psychological conditions. It could explain, for example, why people with an autism spectrum condition sometimes find it hard to read the meaning of other people's behaviour. Conversely, it may help explain why people with schizophrenia are more prone to see meaning and intention where none exists.&quot;</p>
</li>
<li>
<p><em>abstract</em> Humans interpret others’ behaviour as intentional and expect them to take the most energy-efficient path to achieve their goals. Recent studies show that these expectations of efficient action take the form of a prediction of an ideal “reference” trajectory, against which observed actions are evaluated, distorting their perceptual representation towards this expected path. Here we tested whether these predictions depend upon the implied intentionality of the stimulus. Participants saw videos of an actor reaching either efficiently (straight towards an object or arched over an obstacle) or inefficiently (straight towards obstacle or arched over empty space). The hand disappeared mid-trajectory and participants reported the last seen position on a touch-screen. As in prior research, judgments of inefficient actions were biased toward efficiency expectations (straight trajectories upwards to avoid obstacles, arched trajectories downward towards goals). In two further experimental groups, intentionality cues were removed by replacing the hand with a non-agentive ball (group 2), and by removing the action’s biological motion profile (group 3). Removing these cues substantially reduced perceptual biases. Our results therefore confirm that the perception of others’ actions is guided by expectations of efficient actions, which are triggered by the perception of semantic and motion cues to intentionality.</p>
</li>
</ul>
</li>
</ul>
<h2>08049</h2>
<ul>
<li>
<p>ingroup outgroup</p>
</li>
<li>
<p><em>contextual knowledge provided by a movie biases implicit perception of the protagonist</em><br />
mamdooh afdile et al. 2019<br />
<a href="http://dx.doi.org/10.1093/scan/nsz028">http://dx.doi.org/10.1093/scan/nsz028</a></p>
<ul>
<li>
<p>Peoples' brains are naturally biased towards other people who are the same as them -- a behavioural trait scientists call 'in-group favouritism'. The opposite trait is also true: people are often naturally biased against people who are not the same as them, called 'out-group derogation'. Mamdooh Afdile -- a filmmaker studying for a PhD in neuroscience at Aalto University -- decided to use cinema to explore this.</p>
<p>Afdile used the film Priest to create a 20-minute stimulus film version that explored biases in two social groupings: heterosexual and homosexual men. 'If knowledge gained from our social environment can implicitly bias how we perceive each other, this should hold true to characters in movies as well,' Afdile explained. To see if watching the movie biased the viewers subconsciously, Afdile flashed the face of the protagonist repeatedly for a brief duration of 40 milliseconds before and after showing the movie.</p>
<p>Even though the viewer wouldn't be able to notice being shown a person's face -- much less have time to recognise the person -- their subconscious brain responded to the flashed face based on whether or not they had become biased. By using functional MRI, the researchers were able to detect how people's biases could be changed.</p>
<p>In the beginning of the movie, the viewer gets the impression that the priest is heterosexual and falling in love with a woman. At the 10 minute mark, the viewer finds out the priest is in fact in love with another man. The study groups watching the film consisted of 14 homosexual and 15 heterosexual men, and the team measured the bias felt by each group towards the priest character when they thought he was straight, and when they knew he was gay.</p>
<p>The social groupings were chosen by the researchers because, unlike race or gender, we cannot perceive another person's sexual orientation just by looking at their face -- so any bias response by the participants in the experiment toward the face presented to them would be dependent on what they came to know about the person. The subconscious response to the face of the protagonist after seeing the movie, compared to before seeing it, was significantly different between the two groups, and this result was not symmetrical. The results from the heterosexual group showed a very mild negative bias response, and interestingly those from the homosexual group showed a very strong response in brain regions associated with in-group, such as empathy and favouritism.</p>
<p>These results are interesting for our understanding of unconscious bias because they demonstrate that the brain responds in a biased way to traits it can't detect using our basic senses.</p>
<p>'This study shows the brain can be biased based on learned knowledge and not only by external factors,' explains. Mamdooh Afdile. By combining movies with subliminal measurement we can now investigate the subconscious brain in ways that were extremely difficult before.'</p>
</li>
<li>
<p><em>abstract</em> We are constantly categorizing other people as belonging to our in-group (“one of us”) or out-group (“one of them”). Such grouping occurs fast and automatically and can be based on others’ visible characteristics such as skin color or clothing style. Here we studied neural underpinnings of implicit social grouping not often visible on the face, male sexual orientation. Fourteen homosexuals and 15 heterosexual males were scanned in functional MRI while watching a movie about a homosexual man, whose face was also presented subliminally before (subjects did not know about the character’ sexual orientation) and after the movie. We discovered significantly stronger activation to the man’s face after seeing the movie in homosexual but not heterosexual subjects in medial prefrontal cortex (mPFC), frontal pole (FP), anterior cingulate cortex (ACC), right temporal parietal junction (rTPJ) and bilateral superior frontal gyrus (sFG). In previous research, these brain areas have been connected to social perception, self-referential thinking, empathy, theory of mind, and in-group perception. In line with previous studies showing biased perception of in/out-group faces to be context dependent, our novel approach further demonstrates how complex contextual knowledge gained under naturalistic viewing can bias implicit social perception.</p>
</li>
</ul>
</li>
</ul>
<h2>08149</h2>
<h2>08249</h2>
<h2>08349</h2>
<h2>08449</h2>
<h2>08549</h2>
<h2>08649</h2>
<h2>08749</h2>
<ul>
<li>
<p><em>unconventional consumption methods and enjoying things consumed: recapturing the “first-time” experience</em><br />
ed o’brien, robert w. smith 2018<br />
<a href="http://dx.doi.org/10.1177/0146167218779823">http://dx.doi.org/10.1177/0146167218779823</a></p>
<ul>
<li>People commonly lament the inability to re-experience familiar things as they were first experienced. Four experiments suggest that consuming familiar things in new <em>ways</em> can disrupt adaptation and revitalize enjoyment. Participants better enjoyed the same familiar food (Experiment 1), drink (Experiment 2), and video (Experiments 3a-3b) simply when re-experiencing the entity via unusual means (e.g., eating popcorn using chopsticks vs. hands). This occurs because unconventional methods invite an immersive “first-time” perspective on the consumption object: boosts in enjoyment were mediated by revitalized immersion into the consumption experience and were moderated by time such that they were strongest when using unconventional methods for the first time (Experiments 1-2); likewise, unconventional methods that actively disrupted immersion did not elicit the boost, despite being novel (Experiments 3a-3b). Before abandoning once-enjoyable entities, knowing to consume old things in new <em>ways</em> (vs. attaining new things altogether) might temporarily restore enjoyment and postpone wasteful replacement.</li>
</ul>
</li>
<li>
<p>“experiences vs things”</p>
<ul>
<li>
<p><em>stuffocation</em><br />
james wallman 20 978-0-9575245-2-1</p>
</li>
<li>
<p>“experiences are more prone to positive reinterpretation, less likely to be dulled by hedonic adaptation, harder to compare, more likely to contribute to identity, and they bring you closer to people.”</p>
</li>
</ul>
</li>
</ul>
<h2>08849</h2>
<h2>08949</h2>
<h2>09049</h2>
<ul>
<li>
<p><em>mechanosensory-based phase coding of odor identity in the olfactory bulb</em><br />
ryo iwata et al. 2017<br />
<a href="http://dx.doi.org/10.1016/j.neuron.2017.11.008">http://dx.doi.org/10.1016/j.neuron.2017.11.008</a></p>
<ul>
<li>
<p>•Mechanosensation in olfactory sensory neurons generates sniff-coupled oscillations<br />
•Phase coding in mitral/tufted cells distinguishes odor from mechanical signals<br />
•Phase coding is more stable than rate coding across time and odor concentrations<br />
•The loss of mechanosensory-based oscillations impairs robust phase coding of odors</p>
<p>Mitral and tufted (M/T) cells in the olfactory bulb produce rich temporal patterns of activity in response to different odors. However, it remains unknown how these temporal patterns are generated and how they are utilized in olfaction. Here we show that temporal patterning effectively discriminates between the two sensory modalities detected by olfactory sensory neurons (OSNs): odor and airflow-driven mechanical signals. Sniff-induced mechanosensation generates glomerulus-specific oscillatory activity in M/T cells, whose phase was invariant across airflow speed. In contrast, odor stimulation caused phase shifts (phase coding). We also found that odor-evoked phase shifts are concentration invariant and stable across multiple sniff cycles, contrary to the labile nature of rate coding. The loss of oscillatory mechanosensation impaired the precision and stability of phase coding, demonstrating its role in olfaction. We propose that phase, not rate, coding is a robust encoding strategy of odor identity and is ensured by airflow-induced mechanosensation in OSNs.</p>
</li>
</ul>
</li>
<li>
<p><em>volatile biomarkers of symptomatic and asymptomatic malaria infection in humans</em><br />
consuelo m. de moraes et al. 2018<br />
<a href="http://dx.doi.org/10.1073/pnas.1801512115">http://dx.doi.org/10.1073/pnas.1801512115</a></p>
<ul>
<li>Malaria elimination efforts are hindered by the prevalence of asymptomatic infections, which frequently go undetected and untreated. Consequently, there is a pressing need for improved diagnostic screening methods. Based on extensive collections of skin odors from human populations in Kenya, we report broad and consistent effects of malaria infection on human volatile emissions. Furthermore, we found that predictive models based on machine learning algorithms reliably determined infection status based on volatile biomarkers and, critically, identified asymptomatic infections with 100% sensitivity, even in the case of low-level infections not detectable by microscopy. These findings suggest that volatile biomarkers have significant potential for the development of robust, noninvasive screening methods for detecting symptomatic and asymptomatic malaria infections under field conditions.</li>
</ul>
</li>
</ul>
<h2>09149</h2>
<ul>
<li>from lecture notes (origin unknown)
<ul>
<li>non-motile primary cilium sensory cells in mammals as mechanoreceptors</li>
<li>• ruffini corpuscles: touch, pressure. slow adaptation. respond to steady displacement.<br />
• hair follicle receptors: hair displacement, rapid adaptation<br />
• meissner corpuscles: touch, vibration, rapid adaptation (velocity<br />
detection) (= krause’s end bulbs in non-primate mammals)<br />
• paccinian corpuscles: touch, vibration, very fast adaption (acceleration detection)</li>
</ul>
</li>
</ul>
<h2>09249</h2>
<h2>09349</h2>
<ul>
<li><em>how cotton production in medieval china unravelled patriarchy</em><br />
melanie meng xue 2018<br />
<a href="https://aeon.co/ideas/how-cotton-production-in-medieval-china-unravelled-patriarchy">https://aeon.co/ideas/how-cotton-production-in-medieval-china-unravelled-patriarchy</a></li>
</ul>
<h2>09449</h2>
<h2>09549</h2>
<h2>09649</h2>
<h2>09749</h2>
<ul>
<li>not yet read
<ul>
<li>
<p><em>champions of illusion: the science behind mind-boggling images and mystifying brain puzzles</em><br />
susana martinez-conde, stephen macknik 2017</p>
</li>
<li>
<p><em>reading people: how seeing the world through the lens of personality changes everything</em><br />
anne bogel 2017</p>
<ul>
<li>author is devout christian?</li>
</ul>
</li>
<li>
<p><em>seeing what others don’t: the remarkable ways we gain insights</em><br />
gary klein 2013</p>
</li>
<li>
<p><em>how to be more interesting</em><br />
edward de bono 2010</p>
</li>
<li>
<p><em>yes! 50 secrets from the science of persuasion</em><br />
noah goldstein, steve martin, robert cialdini 2007</p>
</li>
<li>
<p><em>blindspot: hidden biases of good people</em><br />
mahzarin banaji &amp; anthony greenwald 2013</p>
</li>
<li>
<p><em>race on the brain: what implicit bias gets wrong about the struggle for racial justice</em><br />
jonathan kahn 2017</p>
</li>
<li>
<p><em>is science racist?</em><br />
jonathan marks 2017</p>
</li>
<li>
<p><em>less than human: why we demean, enslave, and exterminate others</em><br />
david livingstone smith 2012</p>
</li>
<li>
<p><em>coming to our senses: perceiving complexity to avoid catastrophes</em><br />
viki mccabe 2014</p>
</li>
<li>
<p><em>war of the worldviews: where science and spirituality meet and do not</em><br />
deepak chopra, leonard mlodinow 2012</p>
</li>
<li>
<p><em>the smell of fresh rain: the unexpected pleasures of our most elusive sense</em> barney shaw 2017</p>
</li>
<li>
<p><em>simplexity: why simple things become complex (and how complex things can be made simple)</em><br />
jeffrey kluger 2008</p>
</li>
<li>
<p><em>our senses: an immersive experience</em><br />
rob desalle 2018</p>
</li>
<li>
<p><em>the power of intuition: how to use your gut feelings to make better decisions at work</em><br />
gary klein 2004</p>
</li>
<li>
<p><em>the mind is flat: the illusion of mental depth and the improvised mind</em><br />
nick chater 2018</p>
</li>
<li>
<p><em>the forgetting machine: memory, perception, and the “jennifer aniston neuron”</em><br />
rodrigo quian quiroga 2017</p>
</li>
<li>
<p><em>the intuitive way the definitive guide to increasing your awareness</em><br />
penney peirce 1997</p>
</li>
<li>
<p><em>the confidence game: the psychology of the con and why we fall for it every time</em><br />
konnikova maria 2017</p>
</li>
<li>
<p><em>joyful: the surprising power of ordinary things to create extraordinary happiness</em><br />
ingrid fetell lee 2018</p>
</li>
<li>
<p><em>the perils of perception: why we’re wrong about nearly everything</em><br />
bobby duffy 2018</p>
</li>
</ul>
</li>
</ul>
<h2>09849</h2>
<h2>09949</h2>
<h2>04949</h2>
<h2>04849</h2>
<ul>
<li>Huxley had a similar–sounding visual recall issue as I have.</li>
</ul>
<h2>04749</h2>
<h2>04649</h2>
<h2>04549</h2>
<h2>04449</h2>
<h2>04349</h2>
<h2>04249</h2>
<h2>04149</h2>
<h2>04049</h2>
<h2>03949</h2>
<h2>03849</h2>
<h2>03749</h2>
<h2>03649</h2>
<h2>03549</h2>
<h2>03449</h2>
<h2>03349</h2>
<h2>03249</h2>
<h2>03149</h2>
<h2>03049</h2>
<h2>02949</h2>
<h2>02849</h2>
<h2>02749</h2>
<h2>02649</h2>
<h2>02549</h2>
<h2>02449</h2>
<h2>02349</h2>
<h2>02249</h2>
<h2>02149</h2>
<h2>02049</h2>
<h2>01949</h2>
<h2>01849</h2>
<h2>01749</h2>
<h2>01649</h2>
<h2>01549</h2>
<h2>01449</h2>
<h2>01349</h2>
<h2>01249</h2>
<h2>01149</h2>
<h2>01049</h2>
<h2>00949</h2>
<h2>00849</h2>
<h2>00749</h2>
<ul>
<li>a person can be greatly aware one moment but almost unaware the next</li>
</ul>
<h2>00649</h2>
<ul>
<li>there are no absolutes, no truly right, no truly wrong. we should not judge ourselves or others harshly — for even our best intentions and plans may not be as close to responsible as we thought they were.</li>
</ul>
<h2>00549</h2>
<ul>
<li>
<p><em>world too small</em><br />
when someone's world becomes too small, that is when problems start. They start to attribute even accidental consequences to alleged malicious behaviour in the people around them.</p>
</li>
<li>
<p>our perspective sometimes narrows, and it is then that we are more likely to abuse.</p>
</li>
<li>
<p>when our perspective narrows, it becomes harder to break out of our current outlook, to learn of the changing shape of reality, to look beyond what we already believe.</p>
</li>
</ul>
<h2>00449</h2>
<ul>
<li>dyslexia
<ul>
<li>dyslexic in dream: I could read the numbers but couldn't quite make out the letters; apparently this is what happens in dyslexia. could it be that in dreams I am dyslexic? apparently not all people are dyslexic in their dreams, at least not all the time. so could it be that when a certain part of the brain is resting in sleep, it resembles dyslexia?</li>
<li>could it be that dream states are because certain parts of the brain are resting and we may be able to correlate different types of dream states with different parts of the brain resting?</li>
<li>maybe dyslexics have a certain kind of synaesthesia which mixes letters with colours and movement. when colorised filters applied, the colour no longer salient so movement also suppressed. the type of coloured filter required to block the so-called dyslexia varies with the kind of so-called synaesthesia.</li>
</ul>
</li>
</ul>
<h2>00349</h2>
<ul>
<li>Instinct
<ul>
<li>I have a strange premonition that something is going to happen soon in the world, and we won't be able to contact each other. I want you to know I will be thinking of you. here are my notes so far, I don't know if they'll be any help…</li>
<li>when I was young, I felt intuitively that something was awfully wrong with our society. after another twenty years I understood how to communicate this in language</li>
<li>trust your instincts. if they tell you to be wary, that's okay.</li>
<li>Instinct is a name given to processes we do not yet understand. Perception may be a better name for it. We can perceive and act on a situation even if we don't understand how to explain it in words.</li>
<li>Test your perception and then trust it. A way to approach this is to try to understand the physical filters on your perception, and your mental filters such as bias and judgement. Then we can understand some of the obstacles in the way of our perception and can trust our own perception.</li>
<li>people minimise what they don't understand. most want conscious explanation of hunches, feelings, premonitions before they can actually be articulated by our conscious brains, that is the very definition of premonition. they devalue this because they don't understand it and don't trust that we can understand something is useful when they can't. they demand that we trust them but they need to uphold the corresponding reponsibility. at the moment they don't so it is why we don't like it even though we could not find the words to express it before
<ul>
<li>a crucial difference in outlook and expectation that exists and is self–perpetuating</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>00249</h2>
<ul>
<li>
<p><em>see–feel–value</em><br />
literally cannot see or feel something (like risk, climate change), so most people can not value it</p>
</li>
<li>
<p>if we cannot “see” or perceive something, then we have little hope of understanding or changing it.</p>
</li>
</ul>
<h2>00149</h2>
<ul>
<li>our internal construction of the world is biased towards an internal cohesion — whether it is a sense of time flow, effectiveness or perceived utility. so for example, we are much less effective at reading and absorbing information when we are tired — but we rarely notice how very tired we are, or understand how slow we get when we are very sleepy, nor understand how far we are underperforming when in that state — we believe we are fully competent almost all the way up to falling asleep; and if we don’t notice such an obvious state, how does might we expect to deal with more subtle states?</li>
<li>just like we can’t detect our own impairment when we are tired, we also do not detect our impairment due to cognitive decline or misjudgement. we must use indirect methods to correct for this, such as the response of peers (or of the environment, in the case of evolution).</li>
<li>similarly, hypothermic, memory, age, almost everything, if you understand it rather than relying on our current internal representation of it. so far have not found any area of human competency that stands up to this kind of scrutiny, not a one. but it doesn’t mean we are a lost cause — probability helps us out here, through diversity and evolution.</li>
</ul>
<h2>00049</h2>
<ul>
<li>indium In (indigo, from blue spectrum lines)</li>
<li>candy smith–foster
<ul>
<li>crossing a railway bridge</li>
<li>parrot</li>
</ul>
</li>
<li>DI</li>
</ul>

</div>

</body>
</html>
